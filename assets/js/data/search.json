[
  
  {
    "title": "Cpp Blog: Templates Part 2",
    "url": "/posts/templates-p2/",
    "categories": "Cpp Blog",
    "tags": "cpp",
    "date": "2020-11-05 00:00:00 +0800",
    





    "snippet": "Back to Basics: Templates Part 2These are my notes from the second part of Andreas Fertig ‘s talk from CppCon 2020 titled “Back to Basics: Templates”.Variadic TemplatesVariadic templates allow you to pass a variable number of arguments to a template. It is denoted using three dots ....In the example below, min continuously expands the items in the initializer list while finding the min until there are two items left.template &lt;typename T, typename... Ts&gt;constexpr auto min(T&amp; a, T&amp;b, const Ts&amp;... ts) {    const auto m = a &lt; b ? a : b;    if constexpr(sizeof...(ts) &gt; 0 ) {        return min(m, ts...);    }    return m;}static_assert( min(3,2,3,4,5) == 2);static_assert( min(3,2) == 2);Folded ExpressionsFolded expressions are used to unpack a parameter pack using an operations. There are two four types of folded expressions listed below. The op denotes some operation of that is of the same type.unary  right fold (pack op …)  left fold (… op pack)binary  right fold (pack op … op init)  left fold (init op … op pack)In the example below, we use a unary left fold to unpack our parameter pack into std::cout.template&lt;typename T, typename... Ts&gt;void Print(const T&amp; targ, const Ts&amp;... args){  std::cout &lt;&lt; targ;  aut coutSpaceAndArg = [](const auto&amp; arg) {    std::cout &lt;&lt; ' ' &lt;&lt; arg;  };  (..., coutSpaceAndArg(args));}Print(\"hello\", \"C++\", 20);Variable TemplatesUsing variable templates we can define constants such as pi or true_type. The example below illustrates the power of variable templates. Using the integral_constant struct we can define a true_type and a false_type. We then define two templates is_pointer that will derive from false_type or true_type depending on its input. In doing so, we can have a compile time check for the type of a variable.template&lt;class T, T v&gt;struct integral_constant {    static constexpr T value = v;};using true_type = integral_constant&lt;bool, true&gt;;using false_type = integral_constant&lt;bool, false&gt;;// derive from integral_constant types abovetemplate&lt;class T&gt;struct is_pointer : false_type{};template&lt;class T&gt;struct is_pointer&lt;T*&gt; : true_type {};static_assert(is_pointer&lt;int*&gt;::value);static_assert(not is_pointer&lt;int&gt;::value);SFINAESubstitution failure is not and error (SFINAE) is a technique that gives the compiler hints as to how to compiler our code if its initial subsitution fails.In this example our compiler decides on the function to instantiate not on the raw type of T, but rather on the result of std::is_floating_point_v.template &lt;typename T&gt;std::enable_if_t&lt;not std::is_floating_point_v&lt;T&gt;, bool&gt; equal(const T&amp; a, const T&amp; b) {    return a == b;}std::enable_if_t&lt;std::is_floating_point_v&lt;T&gt;, bool&gt; equal(const T&amp; a, const T&amp; b) {    return std::abs(a - b) &lt; 0.0001;}equal(2, 1);equal(2.0, 1.0);Tag DispatchTag dispatch is an alternative to SFINAE. In this technique, we define empty class tags and pass them to our functions as additional parameters. In doing so, we can overload functions with identical parameters.In the example below, we define two empty structs notFloatingPoint and floatingPoint which we pass to the equal function to distinguish between the two.namespace internal {    struct notFloatingPoint {};    struct floatingPoint {};    template &lt;typename T&gt;    bool equal(const T&amp; a, const T&amp; b, notFloatingPoint)  {        return a == b;    }    template &lt;typename T&gt;    bool equal(const T&amp; a, const T&amp; b, notFloatingPoint)  {        return std::abs(a - b) &lt; 0.0001;    }} // namespace internaltemplate&lt;typename T&gt;bool equal(const T&amp; a, const T&amp; b) {    using namespace internal;    if constexpr (std::is_floating_point_v&lt;T&gt;) {        return equal(a, b, floatingPoint{});    } else {        return equal(a, b, notFloatingPoint{});    }}C++20 ConceptsUsing Concepts in C++20, we can use the requires keyword to replace SFINAE.template&lt;typename T&gt;requires(not std::is_floating_point_v&lt;T&gt;) bool equal(const T&amp; a, const T&amp; b) {    return a == b;}template&lt;typename T&gt;requires(std::is_floating_point_v&lt;T&gt;) bool equal(const T&amp; a, const T&amp; b) {    return std::abs(a - b) &lt; 0.0001;}Template Template ParameterYou can also pass templates to a template. In the example below we use template&lt;class, class&gt; to speciy that this is a template template parameters and then we proceed defining the template as usual.template&lt;    template&lt;class, class&gt;    class Container,    class T,    class Allocataor = std::allocator&lt;T&gt;&gt;void Fun(const Container&lt;T, Allocator&gt;&amp; c) {    for (auto&amp; e : c) {        printf(\"%d\\n\", e);    }}std::vector&lt;int&gt; v {2, 3, 3};Fun(v);std::list&lt;char&gt; li {'a', 'b'};Fun(li);"
  },
  
  {
    "title": "Cpp Blog: Templates",
    "url": "/posts/templates/",
    "categories": "Cpp Blog",
    "tags": "cpp",
    "date": "2020-11-04 00:00:00 +0800",
    





    "snippet": "Back to Basics: Templates Part 1These are my notes from Andreas Fertig ‘s talk from CppCon 2020 titled “Back to Basics: Templates”.IntroductionGeneric programming involves writing algorithms in terms of types that are to be specified later. Templates are commonly used for generic programming tasks in C++.In C++ there are 3 types of templates  function templates  class templates  variable templatesTemplates can take in 3 types of parameters  type parameter (e.g. int, char or even class)  non-type parameter (e.g. typically values like 3)  template-template parameter (required when we pass a template as a parameter to a template)Here is a simple example of a simple function template for Size. Our template takes T a type parameter and N a non-type parameter. This function takes in a raw buffer and returns its size. Our template function will instantiate a new function for different combinations of input types. In this case, two instantiations will be made (one for each N).template &lt;typname T, size_t N&gt;constexpr auto Size(const T (&amp;)[N]) {  return N;}char buffer[16]{};static_assert(Size(buffer) == 16);char buffer2[32]{};static_assert(Size(buffer2) == 32);Template SpecializationTemplate specialization means providing a concrete implementation for some argument combination of a function template. This technique is often used for templates that use integer types and floating point types since different techniques are required to check the equality of the two.The equal function below illustrates a template specialization for double.template&lt;typename T&gt;bool equal(const T&amp; a, const T&amp; b) {  return a==b;}template&lt;&gt;bool equal(const double&amp;a, const double&amp; b) {  return std::abs(a-b) &lt; 0.00001;}static_asssert(equal(1, 2) == false); // instantiate firststatic_asssert(equal(1.0, 2.0) == false); // instantiate secondClass TemplatesSimilar to functions, classes can also be implemented as templates. Methods for our call templates can be implemented inside or outside of the class declaration.In the class Array below, we define every function outside inside of the class with the exception of T* data();. In order to define a class method outside of the class, we need to explicitly state that it’s a template as shown below.template&lt;typename T, size_t SIZE&gt;struct Array {  T* data();  const T* data() const {    return std::addressof (mData[0]);  }  constexpr size_t size() const {    return SIZE;  }  T* begin() {    return data();  }  T* end() {    return data() + size();  }  T&amp; operator[](size_t idx) {    return mData[idx];  }  T mData[SIZE];};// defining method outside of classtemplate&lt;typename T, size_t SIZE&gt;T* array&lt;T, SIZE&gt;::data() {  return std::addressof(mData[0]);}Method templatesMethods inside a class can themselves be templates as well. In the example below, we overload the operator= of the class Foo that allows us to cast other types U to T.template&lt;typename T&gt;class Foo {private:    T mX;public:    Foo(const T&amp; x) : mX{x} {}    template&lt;typename U&gt;    Foo&lt;T&gt;&amp; operator=(const U&amp; u) {        mX = static_cast&lt;T&gt;(u);        return *this;    }};Foo&lt;int&gt; fi{3};fi = 2.5;Inheritance in Class TemplatesSimilar to vanilla classes, template classes can inherit from one another. In order to call methods from the base class in the derived class, we can use the this pointer or make the name known using Base&lt;T&gt;::func.In the example below, the class Bar calls on it’s base class Foo using the aforementioned techniques.template&lt;typename T&gt;class Foo {public:    void Func() {}};template&lt;typename T&gt;class Bar : public Foo&lt;T&gt; {public:    void BarFunc() {        // Func(); THIS WON'T WORK        this-&gt;Func();        Foo&lt;T&gt;::Func();    }}Bar&lt;int&gt; b{};b.BarFunc();Alias TemplatesAlias templates allow you to create synonyms for templates and partial specialization of templatesIn the example below, we see that CharArray is an alias template for std::array using char and of size N.#include &lt;array&gt;template&lt;size_t N&gt;using CharArray = std::array&lt;char, N&gt;;CharArray&lt;25&gt; arr;std::spanstd::span is a helpful addition to C++20 that helps us reduce code bloat. std::span is a class template that is used to store a contiguous sequence of objects as well as its length.In the first version of Send defined below, everytime we call Send with a different N we instantiate a new version of Send increasing your binary size. Using std::span as a “wrapper” around std::array, our compiler will only instantiate one version of Send, thus reducing code bloat.// BEFOREtemplate&lt;size_t N&gt;bool Send(const std::array&lt;char, N&gt;&amp; data) {    return write(data.data(), data.size());}// AFTERbool Send(const span&lt;char&gt;&amp; data) {    return write(data.data(), data.size());}// can call bothstd::array&lt;char, 1'024&gt; buffer();Send(buffer);Constexpr ifConstexpr ifs are compile time conditional statements. Only the branches that yield true are preserved in the final binary.In the example below, we define a generic getValue function that will get the value of a variable based on whether or not it is a pointer type.template &lt;typname T&gt;auto getValue(T t) {    if constexpr(std::is_pointer_v&lt;T&gt;) {        assert(nullptr != t);        return *t;    } else {        return t;    }}int i = 4;int *ip = &amp;i;auto iv = getValue(i);auto ipv = getValue(ip);auto itv = getValue(43);"
  },
  
  {
    "title": "Cpp Blog: Conccurency",
    "url": "/posts/concurrency/",
    "categories": "Cpp Blog",
    "tags": "cpp",
    "date": "2020-11-01 00:00:00 +0800",
    





    "snippet": "Back to Basics: ConcurrencyThese are my notes from Arthur O’Dwyer’s talk from CppCon 2020 titled “Back to Basics: Concurrency”.Thread-safe static initializationSince C++11, the initialization of static variables is guaranteed to be thread-safe. After the first thread’s arrival the other threads will block and wait for the first to succeed or fail with an exception. This technique can be useful for singleton classesinline auto&amp; SingletonFoo::getInstance() {    static SingletonFoo instance;    return instance;}Initializing Members with once_flagFor non-static members initialization, we can ensure thread safety using the once_flag. This flag has a free function std::call_once that you can use to initialize a member variable.The example below illustrates how you could use the once_flag to initialization the conn_ variable by passing a lambda to std::call_once.class Logger {    std::once_flag once_;    std::optional&lt;NetworkConnection&gt; conn_;    NetworkConnection&amp; getConn() {        std::call_once(once_, []{            conn_ = NetworkConnection(defaultHost);        });        return conn_;    }};shared_mutex reader/writer lockThere are two ways to lock a shared_mutex. Using unique_lock as in void set(...) below will acquire an exclusive/writer lock. Using shared_lock as in int get(...) below you will acquire a reader lock meaning.class ThreadSafeConfig {    std::map&lt;std::string, int&gt; settings_;    mutable std::shared_mutex rw_;    void set(const std::string&amp; name, int value) {        std::unique_lock&lt;std::shared_mutex&gt; lk(rw_);        settings_.insert_or_assign(name, value);    }    int get(const std::string&amp; name) const {        std::shared_lock&lt;std::shared_mutex&gt; lk(rw_);        return settings_.at(name);    }};C++20 counting_semaphoreSemaphore is thread safe counter; O’Dwyer likens it to a bag of chips. .acquire() removes a chip from the bag. If there are no chips available, it will block until a chip is available. release() returns a chip to the bag.class AnonymousTokenPool {    std::counting_semaphore&lt;256&gt; sem_{100}; // 256 = max count, 100 = initial counter value    void getToken() {        sem_.acquire();    }    void returnToken() {        sem_.release();    }};A more RAII based approach to the token class above is the wrap the semaphores in a unique_ptr that calls .release() in its destructor.using Sem = std::counting_semaphore&lt;256&gt;;struct SemReleaser {    bool operator()(Sem *s) const { s-&gt;release(); }};class AnonymousTokenPool {    Sem sem_{100};    using Token = std::unique_ptr&lt;Sem, SemReleaser&gt;;    Token borrowToken() {        sem_.acquire();        return Token(&amp;sem);    }};C++20 std::latchLatch is a threadsafe counter that starts positive and counts down to zero. It is gate that waits for everyone to arrive at this point, then unblocks everyone simultaneously. It supports three main functions:  latch.wait() -&gt; block until counter reaches 0  latch.count_down() -&gt; decrements the counter, if counter reaches zero, we unblock all waiters  latch.arrive_and_wait() -&gt; decrements and begins waitingHere is an example of using a latch to synchronize thread outputs.std::latch myLatch(1);std::thread threadB = std::thread([&amp;](){    myLatch.wait();    std::cout &lt;&lt; \"hello B\" &lt;&lt; std::endl});std::cout &lt;&lt; \"hello A\" &lt;&lt; std::endlmyLatch.arrive();threadB.join();std::cout &lt;&lt; \"hello A again\" &lt;&lt; std::endlC++20 std::barrierBarrier is a resettable latch. Once the the counter reaches 0, it unblocks all threads waiting for it and atomically refreshes the counter. std::barrier exposes a .wait(), .count_down() and .arrive_and_wait() function as with latches.In the code snippet below, you should see A and B both say that they are setting up, green flag then A and B is running.std::barriar b(2, []{ puts(\"green flag, go!\"); } );std::thread threadB = std::thread([&amp;](){    std::cout &lt;&lt; \"B is setting up\" &lt;&lt; std::endl    b.arrive_and_wait();    std::cout &lt;&lt; \"B is running\" &lt;&lt; std::endl});std::cout &lt;&lt; \"A is setting up\" &lt;&lt; std::endlb.arrive_and_wait();std::cout &lt;&lt; \"A is running\" &lt;&lt; std::endlthreadB.join();"
  },
  
  {
    "title": "Cpp Blog: How are executables made?",
    "url": "/posts/how-are-programs-compiled/",
    "categories": "Cpp Blog",
    "tags": "cpp",
    "date": "2020-10-31 00:00:00 +0800",
    





    "snippet": "How are executables made?The process of creating an executable can be broken down into a compiling and a linker stage.The stages of compiling are:  pre-processing  linguistic analysis  assembling  optimization  code emissionThe stages of linking are:  relocation  reference resolvingCompilingThis input to the compiler is a translation unit; this is a text file containing source code (e.g. SOMETHING.cpp). The compiler output will be a collection of binary object files, one for each translational unit.Pre-processingAs suggusted from its name, this stage deals with preprocessor macros. This includes turning#define statements into constants, converting macro definitions into code where needed, conditionally excluding code based on #if, #elif, and #endif directives.Here are some examples of code that the preprocessor would deal with.# define SOME_CONSTANT 1int i = SOME_CONSTANT + 1;#ifdef testint someFunction() {    return 0;}#endifLinguistic AnalysisThis is also known as the “complainer stage”. This stage is further broken down into three stages:      Lexical AnalysisThis means breaking down the source code into non-divisible tokens. This means breaking down your source code into keywords (e.g. int, double, float), separators (e.g. ;), operators (e.g. +, -), identifiers (e.g. a variable x), literals (e.g. 4) etc.        Parsing/Syntax AnalysisThis state concatenates the extracted tokens inthe chains of tokens and verifies if the ordering makes sense to the programming language. Some examples of C syntax rules include: separating statements with semicolon, ensuring conditional if statements are enclosed with curly braces.        Semantic AnalysisSemantic analysis involves discovering whether the syntactically correct statements actually make sense. For instance, suppose the compiler saw the line: ++x. If x were an int, the behaviour would be defined. However, if x were a float the behaviour would be undefined.  AssemblingHere, the compiler will turn the code into CPU instructions. suppose you had a file function.c, on x86 platforms you can view it’s assembly output in Intel or AT&amp;T format using:$ gcc -S -masm=att function.c -o function.s$ gcc -S -masm=intel function.c -o function.sOptimizationIn this stage, the compiler tries to minimize its use of registers and delete any uneeded code (e.g. unused variables).Code EmissionAt this stage, the assembly instructions will be converted into binary values corresponding to machine instructions (also known as opcodes) and written to their specific object files.LinkingRelocationAt this point, you have a collection of object files that need to be combined into one single program. The relocation stage moves the zero-based address ranges of the object file code into the address ranges of resultant program memory map.Resolving ReferencesWhen translational units reference each other, they do not know where in the final program these variables or functions will be located. With all of the required code pasted into the final program map, the linker can now replace the dummy addresses use for external references in translational units with the concrete address of the variables/functions.Sources  C and C++ Compiling by Milan Stevanovic"
  },
  
  {
    "title": "Cpp Blog: Virtual Destructors",
    "url": "/posts/virtual-destructors/",
    "categories": "Cpp Blog",
    "tags": "cpp",
    "date": "2020-10-29 00:00:00 +0800",
    





    "snippet": "Virtual DestructorsAs a general rule of thumb you should always make the destructor of your base class virtual. Why is that?Consider the following piece of code which defines a Base and a Derived class.class Base {public:    Base() {        cout &lt;&lt; \"Base Constructor\" &lt;&lt; endl;    }    ~Base() {        cout &lt;&lt; \"Base Destructor\" &lt;&lt; endl;    }};class Derived : public Base {public:    Derived() {        cout &lt;&lt; \"Derived Constructor\" &lt;&lt; endl;    }    ~Derived() {        cout &lt;&lt; \"Derived Destructor\" &lt;&lt; endl;    }};int main(){    Base* b = new Derived();    delete b;}This piece of code will print:Base ConstructorDerived ConstructorBase DestructorBecause the variable b was of type Base*, we only called the destructor of Base. If the Derived class were to dynamically allocate memory in its constructor, our call to delete b would not cleanup all heap allocations as desired.To correct this behaviour we need to declare the destructor of Base as virtual.class Base {public:    Base() {        cout &lt;&lt; \"Base Constructor\" &lt;&lt; endl;    }    virtual ~Base() {        cout &lt;&lt; \"Base Destructor\" &lt;&lt; endl;    }};class Derived : public Base {public:    Derived() {        cout &lt;&lt; \"Derived Constructor\" &lt;&lt; endl;    }    ~Derived() {        cout &lt;&lt; \"Derived Destructor\" &lt;&lt; endl;    }};int main(){    Base* b = new Derived();    delete b;}This piece of code prints:Base ConstructorDerived ConstructorDerived DestructorBase DestructorThis is the desired behaviour that we want. So in general when dealing with polymorphic types in C++, make sure your destructors are virtual.Sources:  https://stackoverflow.com/questions/461203/when-to-use-virtual-destructors  http://www.gotw.ca/publications/mill18.htm  https://www.geeksforgeeks.org/virtual-destructor/"
  },
  
  {
    "title": "Robotics Blog: Unscented Kalman Filter",
    "url": "/posts/ukf/",
    "categories": "Robotics Blog",
    "tags": "robotics",
    "date": "2020-10-20 00:00:00 +0800",
    





    "snippet": "Unscented Kalman FilterThe unscented transform can be briefly summarized in 3 steps:  Draw N deterministic samples from an input PDF.  Transform these samples through a nonlinear function.  Use the transformed points to combine the mean and covariance of the output PDF.This examples uses a similar motion model to the EKF example in the previous post.State PredictThe state predict is a 3 step procedure  Generate sigma points  Propogate points through motion model  Calculate mean and covariance of sigma pointsState UpdateThe state update is a similar 3 step procedure  Generate sigma points  Propogate points through observation model  Calculate mean and covariance of sigma pointsGenerating Sigma PointsFor a state with L dimensions, we need to generate 2L + 1 sigma points. We assume our state is Gaussian with a mean and covariance\\[\\mathcal{N} (\\mu*x, \\Sigma*{xx})\\]We take the cholesky decomposition of our covariance matrix to obtain a bolded L matrix.\\[\\boldsymbol{L}\\boldsymbol{L}^T = \\Sigma\\_{xx}\\]Sigma point \\(x_i\\) can be represented below\\[x_0 = \\mu_x\\]\\(x_i = \\mu_x + \\sqrt{(L + \\lambda ) col_i \\boldsymbol{L}}\\) for \\(i = 1, ..., L\\)\\(x_{i} = \\mu_x - \\sqrt{(L + \\lambda ) col_i \\boldsymbol{L}}\\)  for \\(i = L + 1, ..., 2L\\)This formula is reflected in the generateSigmaPoints function. We use\\[\\gamma = \\sqrt{L + \\lambda}\\] std::vector&lt;Eigen::Vector4f&gt; generateSigmaPoints(     const Eigen::Vector4f&amp; x_est, const Eigen::Matrix4f&amp; P_est, const float gamma) {    const size_t L = 4;    std::vector&lt;Eigen::Vector4f&gt; sigma_points;    sigma_points.reserve(2*L + 1);    sigma_points.push_back(x_est);    // do cholesky    Eigen::Matrix4f P_sqrt ( P_est.llt().matrixL() );    for (size_t i = 0; i &lt; L; ++i) {        Eigen::Vector4f pt = x_est + gamma * P_sqrt.col(i);        sigma_points.push_back(pt);    }    for (size_t i = 0; i &lt; L; ++i) {        Eigen::Vector4f pt = x_est - gamma * P_sqrt.col(i);        sigma_points.push_back(pt);    }    return sigma_points;}Recovering Mean and CovarianceAfter we pass our sigma points through some nonlinear function \\(y = f(x_i)\\), where \\(f\\) can be your motion/observation model we need to compute the mean and covariance of the new set of sigma points.Our weight vector in recovering our mean is\\[w_{m, i} =\\left\\{\\begin{matrix}\\frac{ \\lambda }{\\lambda+ L} &amp; i = 0\\\\\\frac{ 1 }{2 (L + \\lambda)}&amp; i = 1, ..., 2L\\end{matrix}\\right.\\]Our weight vector in recovering our covariance is\\[w_{c, i} =\\left\\{\\begin{matrix}\\frac{ \\lambda }{\\lambda+ L}  + ( 1 - \\alpha^2  + \\beta) &amp; i = 0\\\\\\frac{ 1 }{2 (L + \\lambda)}&amp; i = 1, ..., 2L\\end{matrix}\\right.\\]std::tuple&lt;std::vector&lt;float&gt;, std::vector&lt;float&gt;, float&gt;    setupWeights(float nx, float alpha, float kappa, float beta) {    // nx = L    float lamb = std::pow(alpha, 2) * (nx + kappa) - nx;    float gamma = std::sqrt(nx + lamb);    std::vector&lt;float&gt; wm(2*nx+1, 0.0f);    std::vector&lt;float&gt; wc(2*nx+1, 0.0f);    wm[0] = lamb / (lamb + nx);    wc[0] = ( lamb / (lamb + nx) ) + (1 - std::pow(alpha, 2) + beta);    for (size_t i = 1; i &lt; 2*nx+1; ++i) {        wm[i] = 1.0f / (2 * (nx + lamb));        wc[i] = 1.0f / (2 * (nx + lamb));    }    return {wm, wc, gamma};}Using our weights we can calculate the mean and covariance as follows:\\[\\mu_y = \\sum w_{m, i} y_i\\]\\[\\Sigma_{yy} = \\sum w_{c, i} (y_i - \\mu_y) (y_i - \\mu_y)^T\\]The summation of the weights with the individual sigma points for the motion model and observation model are very similar. We use a gain matrix K (similar to EKF) to propogate our state estimate.void unscentedKalmanFilter(Eigen::Vector4f&amp; x_est, Eigen::Matrix4f&amp; P_est,        const Eigen::Vector2f&amp; u, const Eigen::Vector2f&amp; z,        const Eigen::Matrix4f&amp; Q, const Eigen::Matrix2f&amp; R,        const float dt, const float gamma,        const std::vector&lt;float&gt;&amp; wm,        const std::vector&lt;float&gt;&amp; wc) {    std::vector&lt;Eigen::Vector4f&gt; sigma_points;    // STATE PREDICT    // generate and propogate sigma points through motion model    sigma_points = generateSigmaPoints(x_est, P_est, gamma);    for (auto&amp; pt : sigma_points)        pt = motionModel(pt, u, dt);    // recombine sigma points to get predict mean and cov    Eigen::Vector4f x_pred = Eigen::Vector4f::Zero(4);    for (size_t i = 0; i &lt; sigma_points.size(); ++i)        x_pred += wm[i] * sigma_points[i];    Eigen::Matrix4f P_pred = Q;    for (size_t i = 0; i &lt; sigma_points.size(); ++i) {        Eigen::Vector4f diff = (sigma_points[i] - x_pred);        P_pred += wc[i] * diff * diff.transpose();    }    // STATE CORRECT    Eigen::Vector2f z_pred = observationModel(x_pred);    Eigen::Vector2f y = z - z_pred;    // generate sigma points, transform to observation domain    sigma_points = generateSigmaPoints(x_pred, P_pred, gamma);    std::vector&lt;Eigen::Vector2f&gt; z_sigma_points;    for (auto&amp; pt : sigma_points)        z_sigma_points.emplace_back( observationModel(pt) );    Eigen::Vector2f z_mean = Eigen::Vector2f::Zero(2);    for (size_t i = 0; i &lt; sigma_points.size(); ++i)        z_mean += wm[i] * z_sigma_points[i];    Eigen::Matrix2f cov_zz = R; // st    for (size_t i = 0; i &lt; z_sigma_points.size(); ++i) {        Eigen::Vector2f diff = (z_sigma_points[i] - z_mean);        cov_zz += wc[i] * diff * diff.transpose();    }    // Eigen::Matrix&lt;float, 4, 2&gt; cov_xz;    Eigen::Matrix&lt;float, 4, 2&gt; cov_xz = Eigen::MatrixXf::Zero(4,2);    for (size_t i = 0; i &lt; z_sigma_points.size(); ++i) {        Eigen::Vector2f diffz = (z_sigma_points[i] - z_mean);        Eigen::Vector4f diffx = (sigma_points[i] - x_pred);        cov_xz += wc[i] * diffx * diffz.transpose();    }    auto K = cov_xz * cov_zz.inverse(); // (4x2) * (2,2) =&gt; (4,2)    x_est = x_pred + K * y; // (4,1) + (4,2) * (2,1)    P_est = P_pred - K * cov_zz * K.transpose(); // (4,4) - (4,2)*(2,2)*(2,4)}Sources  https://www.seas.harvard.edu/courses/cs281/papers/unscented.pdf"
  },
  
  {
    "title": "Robotics Blog: Extended Kalman Filter",
    "url": "/posts/ekf/",
    "categories": "Robotics Blog",
    "tags": "robotics",
    "date": "2020-10-19 00:00:00 +0800",
    





    "snippet": "This posts outlines an example of localization using an EKF filter. The model is based on a Python implementation by AtsushiSakai on Github. Implementation.Extended Kalman FilterThe kalman filter is linear algorithm that combines a series of noisy measurements and produces an estimate of unknown variables that tend to be more accurate than if you were to use one estimate. The extended kalman filter is a nonlinear version of the kalman filter in which we linearize the motion model and observation model to propogate our states.The equations that govern the extended kalman filter are as follows:State Predict:\\[X_{Pred} = F X_t + B u_t\\]\\[P_{Pred} = J_F P_t J_F^T + Q\\]State Update:\\[z_{Pred} = H X_{Pred}\\]\\[Y = z - z_{Pred}\\]\\[S = J_H P_{Pred} J_H^T + R\\]\\[K = P_{Pred} J_H^T S^{-1}\\]\\[X_{t+1} = X_{Pred} + KY\\]\\[P_{t+1} = (I - K J_H) P_{Pred}\\]In-depth ExampleState, Control and ObservationLet us model the state, control and observation of a 2D robot as follows:We track the (x, y) position of the robot, heading \\(\\phi\\) and velocity \\(v\\) in its state\\[X_t =\\begin{bmatrix}x_t \\\\y_t \\\\\\phi_t \\\\v_t \\\\\\end{bmatrix}\\]We can send any combination of linear velocity and angular velocity to our robot.\\[u_t =\\begin{bmatrix}v_t \\\\\\omega_t \\\\\\end{bmatrix}\\]Our robot makes GPS-like observatoins to its current location.\\[z_t =\\begin{bmatrix}x_t \\\\y_t \\\\\\end{bmatrix}\\]Motion ModelOur robot will be a differential drive robot, so it’s equations of motion are described as follows:\\[\\dot{x} = v cos(\\phi)\\]\\[\\dot{y} = v sin(\\phi)\\]\\[\\dot{\\phi} = \\omega\\]For a small time step we can update the state with the following eqation:\\[X_{t+1} =\\begin{bmatrix}x_t + v cos \\phi_t \\Delta_t \\\\y_t + v sin \\phi_t \\Delta_t \\\\\\phi_t + \\omega_t \\Delta_t \\\\v_t\\end{bmatrix}\\]We can express this in matrix form\\[X_{t+1} =\\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; 0 \\\\0 &amp; 1 &amp; 0 &amp; 0 \\\\0 &amp; 0 &amp; 1 &amp; 0 \\\\0 &amp; 0 &amp; 0 &amp; 0 \\\\\\end{bmatrix}\\begin{bmatrix}x_t  \\\\y_t  \\\\\\phi_t  \\\\v_t  \\\\\\end{bmatrix} +\\begin{bmatrix}cos \\phi_t \\Delta_t  &amp; 0 \\\\sin \\phi_t \\Delta_t &amp; 0   \\\\0 &amp; \\Delta_t \\\\1 &amp; 0 \\\\\\end{bmatrix}\\begin{bmatrix}v_t \\\\\\omega_t \\\\\\end{bmatrix}\\]\\[X_{t+1} =  F X_{t} + B u_t\\]The jacobian of F is:\\[J_F =\\begin{bmatrix}\\frac{\\partial x}{\\partial x} &amp; \\frac{\\partial x}{\\partial y} &amp; \\frac{\\partial x}{\\partial \\phi} &amp; \\frac{\\partial x}{\\partial v} \\\\\\frac{\\partial y}{\\partial x} &amp; \\frac{\\partial y}{\\partial y} &amp; \\frac{\\partial y}{\\partial \\phi} &amp; \\frac{\\partial y}{\\partial v} \\\\\\frac{\\partial \\phi}{\\partial x} &amp; \\frac{\\partial \\phi}{\\partial y} &amp; \\frac{\\partial \\phi}{\\partial \\phi} &amp; \\frac{\\partial \\phi}{\\partial v} \\\\\\frac{\\partial v}{\\partial x} &amp; \\frac{\\partial v}{\\partial y} &amp; \\frac{\\partial v}{\\partial \\phi} &amp; \\frac{\\partial v}{\\partial v} \\\\\\end{bmatrix} =\\begin{bmatrix}1 &amp; 0 &amp; -v sin \\phi \\Delta_t &amp; cos \\phi \\Delta_t \\\\0 &amp; 1 &amp; v cos \\phi \\Delta_t &amp; sin \\phi \\Delta_t \\\\0 &amp; 0 &amp; 1 &amp; 0 \\\\0 &amp; 0 &amp; 0 &amp; 1 \\\\\\end{bmatrix}\\]Observation ModelThe matrix form of our observation model is:\\[z_t = H x_t\\]\\[z_t =\\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; 0 \\\\0 &amp; 1 &amp; 0 &amp; 0 \\\\\\end{bmatrix}\\begin{bmatrix}x_t \\\\y_t \\\\\\phi_t \\\\v_t\\end{bmatrix} =\\begin{bmatrix}x_t \\\\y_t\\end{bmatrix}\\]The jacobian of our observation model is:\\[J_H =\\begin{bmatrix}\\frac{\\partial x}{\\partial x} &amp; \\frac{\\partial x}{\\partial y} &amp; \\frac{\\partial x}{\\partial \\phi} &amp; \\frac{\\partial x}{\\partial v} \\\\\\frac{\\partial y}{\\partial x} &amp; \\frac{\\partial y}{\\partial y} &amp; \\frac{\\partial y}{\\partial \\phi} &amp; \\frac{\\partial y}{\\partial v} \\\\\\end{bmatrix} =\\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; 0 \\\\0 &amp; 1 &amp; 0 &amp; 0\\end{bmatrix}\\]Doing the FilteringWe propogate our state using the extendedKalmanFilter filter function which is basically identical to the equations given above.void extendedKalmanFilter(Eigen::Vector4f&amp; x_est, Eigen::Matrix4f&amp; P_est,                       const Eigen::Vector2f&amp; u, const Eigen::Vector2f&amp; z,                       const Eigen::Matrix4f&amp; Q, const Eigen::Matrix2f&amp; R,                       const float dt ) {    // state predict    Eigen::Vector4f x_pred = motionModel(x_est, u, dt); // 4x1    Eigen::Matrix4f J_F = jacobianF(x_est, u, dt); //4x4    Eigen::Matrix4f P_pred = J_F * P_est * J_F.transpose() + Q; // 4x4    // state update    Eigen::Vector2f z_pred = observationModel(x_pred);    Eigen::Vector2f y = z - z_pred;    Eigen::Matrix&lt;float, 2, 4&gt; J_H = jacobianH();    Eigen::Matrix2f S = J_H * P_pred * J_H.transpose() + R;    Eigen::Matrix&lt;float, 4, 2&gt; K  = P_pred *  J_H.transpose() * S.inverse();    x_est = x_pred + K * y;    P_est = (Eigen::Matrix4f::Identity()  - K * J_H) * P_pred;}"
  },
  
  {
    "title": "Robotics Blog: State Lattice",
    "url": "/posts/state-lattice/",
    "categories": "Robotics Blog",
    "tags": "robotics",
    "date": "2020-10-18 00:00:00 +0800",
    





    "snippet": "The methods outlined in this post are based on the following paper:  State Space Sampling of Feasible Motions for High-Performance Mobile Robot Navigation in Complex EnvironmentsState Lattice MethodsState lattice methods involve precalculating a set of feasible trajectories for your robot using a given shape parametrization. At runtime, the best trajectory can be calculated quickly based on your cost function (e.g. closeness to obstacles). The three shape parametrizations \\(p_{ss}\\) identified in the paper are uniform, biased and lane state sampling. In this post we will be consideration polar uniform and biased sampling.In all of the sampling methods below each trajectory is represented with a start pose and a goal pose. Using the lookup table for goal poses made using the trajectory generator in our previous post, we can use the \\(\\{s, k_0, k_f\\}\\) as an initial guess in our optimization for a new trajectory.Uniform State SamplingUniform state sampling involves uniformly sampling goal poses in a cone in front of the robot.\\[p_{ss} =\\begin{bmatrix}n_p \\\\n_h \\\\d \\\\\\alpha_{min} \\\\\\alpha_{max} \\\\\\psi_{min} \\\\\\psi_{max} \\\\\\end{bmatrix}\\]\\(n_p\\) is the number samples in the terminal state position and represents the number of points along the arc to sample from.\\(n_h\\) is the terminal heading and represents the number of final orientations to sample from at each sample point.\\(d\\) is the terminal position horizon and represents the radius of the cone.\\(\\alpha_{min}\\) and \\(\\alpha_{max}\\) represent the angular range to sample for terminal positions.\\(\\psi_{min}\\) and \\(\\psi_{max}\\) represent the angular range to sample for terminal headings.The green triangle represents the robot in the diagram below. With \\(n_h = 3\\) you can see that we generate 3 red trajectories to each blue dot.In calculateUniformPolarStates, we first generate a series of angles uniformly and then generate our target trajectory using some simple geometry in sampleStates.// NOTE nxy = np in this codestd::vector&lt;Pose2D&gt; calculateUniformPolarStates(size_t nxy, size_t nh, float d,                        float a_min, float a_max, float p_min, float p_max) {    std::vector&lt;float&gt; angle_samples(nxy);    for (int i = 0; i &lt; nxy; ++i)        angle_samples[i] = i / (nxy - 1.f);    std::vector&lt;Pose2D&gt; states = sampleStates(angle_samples, a_min, a_max, d, p_max, p_min, nh);    return states;}std::vector&lt;Pose2D&gt; sampleStates(std::vector&lt;float&gt;&amp; angle_samples, float a_min,                 float a_max, float d, float p_max, float p_min, size_t nh) {    std::vector&lt;Pose2D&gt; states;    float xf, yf, yawf;    for (float i : angle_samples) {        float a = a_min + (a_max - a_min) * i;        for (int j = 0; j &lt; nh; ++j) {            xf = d * std::cos(a);            yf = d * std::sin(a);            if (nh == 1)                yawf = (p_max - p_min) / 2.f + a;            else                yawf = p_min + (p_max - p_min) * j / (nh - 1.f) + a;            Pose2D final_state {xf, yf, yawf};            states.push_back(final_state);        }    }    return states;}Biased State SamplingThe parametrization for biased state sampling is similar to uniform state sample except we add an extra term \\(n_s\\) which is the number of navigation function samples. This new parameter biases our sampling at points where the global cost function is lower. So more blue dots appear near our goal angle.\\[p_{ss} =\\begin{bmatrix}n_s \\\\n_p \\\\n_h \\\\d \\\\\\alpha_{min} \\\\\\alpha_{max} \\\\\\psi_{min} \\\\\\psi_{max} \\\\\\end{bmatrix}\\]Below is heavily commented version of the calculateBiasedPolarStates code.std::vector&lt;Pose2D&gt; calculateBiasedPolarStates(float goal_angle, int ns, int nxy,                                   int nh, int d,                                   float a_min, float a_max,                                   float p_min, float p_max){    std::vector&lt;float&gt; as(ns-1);    std::vector&lt;float&gt; cnav(ns-1);    for (size_t i = 0; i &lt; ns - 1; ++i) {        as[i] = a_min + (a_max - a_min) * i / (ns - 1.f); // this is the angle used in uniform sampling        cnav[i] = M_PI - std::abs(as[i] - goal_angle); // angle difference between uniformly sampled node and goal angle    }    // sum up all the angle diffs and find max element    float cnav_sum = std::accumulate(cnav.begin(), cnav.end(), 0.f);    float cnav_max = *std::max_element(cnav.begin(), cnav.end());    // normalise the distribution of cnav    for (float&amp; cnavi : cnav) {        cnavi = (cnav_max - cnavi) / (cnav_max * ns - cnav_sum);    }    // cumalative sum    std::vector&lt;float&gt; cumsum_cnav(cnav.size());    std::partial_sum(cnav.begin(), cnav.end(), cumsum_cnav.begin());    // output angles based on cumulative sume    std::vector&lt;float&gt; di;    int li = 0;    for (int i = 0; i &lt; nxy; ++i ) {        for (int ii = li; ii &lt; ns-1; ++ii) {            if ( ii*(1.f/ ns) &gt;= i / (nxy - 1.f) ) {                di.push_back(cumsum_cnav[ii]);                li = ii - 1;                break;            }        }    }    // use sample states as before    auto states = sampleStates(di, a_min, a_max, d, p_max, p_min, nh);    return states;}Lane SamplingLane sampling samples based on the geometry of a road lane.\\[p_{ss} =\\begin{bmatrix}l_{center} \\\\l_{heading} \\\\l_{width} \\\\v_{width} \\\\d \\\\n_{p} \\\\n_{l}\\end{bmatrix}\\]\\(l\\_{center}\\) is the lane centerline.\\(l_{heading}\\) is the lane heading.\\(l_{width}\\) is the lane width.\\(v\\_{width}\\) is the vehicle width.\\(d\\) is the lane horizon.\\(n\\_{p}\\) is the number of lateral offsets to sample from along the center position.\\(n\\_{l}\\) is the number of lanes. In this example we kept this to one.The figure below represents how each variable is used. Our robot (green triangle) seeks to reach the red goal pose (red triangle). The robot is currently in frame a and the target pose is in frame b. \\(l_{heading}\\) parametrized the 2D rotation matrix to transform a point from frame b to frame a.Recall a 2D rotation matrix is parametrized as follows.\\[\\begin{bmatrix}cos(\\theta)  &amp; -sin(\\theta)\\\\sin(\\theta) &amp; cos(\\theta)\\\\end{bmatrix}\\]The function below illustrates how each of these variables are used:std::vector&lt;Pose2D&gt; calculateLaneStates(    float l_center, float l_heading, float l_width,    float v_width, float d, size_t nxy) {    // apply rotation matrix to the points    float xc = d * std::cos(l_heading) - l_center * std::sin(l_heading);    float yc = d * std::sin(l_heading) + l_center * std::cos(l_heading);    // move the center point by a small delta to the left or right    // depending on how much \"wiggle room\" is left in the lane    std::vector&lt;Pose2D&gt; states; states.reserve(nxy);    for (size_t i = 0; i &lt; nxy; ++i) {        float delta = -0.5*(l_width-v_width) + (l_width-v_width) * (i / (nxy - 1.f));        float xf = xc - delta * std::sin(l_heading);        float yf = yc + delta * std::cos(l_heading);        float yawf = l_heading;        Pose2D target_state{xf, yf, yawf};        states.push_back(target_state);    }    return states;}"
  },
  
  {
    "title": "Robotics Blog: Trajectory Generator",
    "url": "/posts/trajectory-generator/",
    "categories": "Robotics Blog",
    "tags": "robotics",
    "date": "2020-10-17 00:00:00 +0800",
    





    "snippet": "In this post we will be creating a simplified example of the trajectory generation method outlined in “Optimal rough terrain trajectory generation for wheeled mobile robots” by Thomas M. Howard and Alonzo Kelly.OverviewThe objective of trajectory generation is to generate a set of controls u to satisfy some constraints on our vehicle C subject to a set of governing differential equations \\(f(x, u, t)\\) describing system dynamics.\\[\\dot x = f(x, u, t)\\]\\[C(x, t) = \\textbf{0}\\]The full process of the algorithm is highlighted in light blue in the diagram below. The algorithm takes in your robot’s initial and final state, control parameters and vehicle model as input. It then outputs the trajectory that the robot should follow as well as the control parameters required to acheive that trajectory.The algorithm iteratively optimizes the control parameters until the robot until the cost (generally defined as the difference between the output trajectory’s final state and the robot’s desired final state) is below a certain threshold.In-depth ExampleInputsLet us assume that we have an Ackerman steering robot with a constant velocity of 3m/s. The initial and final state will be given by \\((x_i, y_i, \\theta_i)\\) and \\((x_f, y_f, \\theta_f)\\).We approximate our steer k over time t with a quadratic function \\(k(t) = a t^2 + b t + c\\) where a, b and c are coefficients that will be determined. We wish find a feasible trajectory from our start to the goal using a quadratic time varying steer.Our optimizer will be given an initial steer \\(k_0\\) and an initial guess of the following variables stored in the TrajectoryParam struct: middle steer \\(k_m\\), final steer \\(k_f\\) and the arc length of the trajectory \\(s\\).struct TrajectoryParam {    float s; // path length    float km; // steer in the middle    float kf;// final steer};We will be using the bicycle model as our vehicle model. So our motion model will be constrained to\\[z' =\\begin{bmatrix}x' \\\\y' \\\\v' \\\\\\phi'\\end{bmatrix}= \\begin{bmatrix}v cos( \\phi)) \\\\v sin( \\phi ))\\\\a \\\\\\frac{ v \\tan{ \\delta } }{L}\\end{bmatrix}\\]\\(d_s\\) is a fixed parameter that determines the number of fixed points along our trajectory to sample in generating a trajectory.Trajectory GenerationThe trajectory generation function is defined in Pose2DTrajectory TrajectoryOptimizer::generateTrajectory(TrajectoryParam&amp; params, float k0).The time horizon \\(t_h\\) our initial trajectory will be given by s divided by our velocity.Using the three pairs of points:\\[(k(t), t): \\{ (k_0, 0), (k_m, \\frac{t_h}{2}), (k_f, t_h) \\}\\]we can recover the coefficients that represent \\(k(t)\\).std::array&lt;float, 3&gt; xk = {0.0f, time_horizon/2.0f, time_horizon};std::array&lt;float, 3&gt; yk = {k0, params.km, params.kf};std::array&lt;float, 3&gt; coeff = quadraticCoefficients(xk, yk);Using the coefficients of \\(k(t)\\) we can then roll out the trajectory generated by this steer command over discretized steps.for (float t = 0.f; t &lt; time_horizon; t += dt) {        float steer = quadraticInterpolation(coeff, t);        state.update(0.0f, steer, dt); // no acc.        traj.x.push_back(state.x);        traj.y.push_back(state.y);        traj.theta.push_back(state.yaw);}Compute CostThe cost calculation is done in Eigen::Vector3f TrajectoryOptimizer::calculatePoseErrorVector(const Pose2D&amp; p1, const Pose2D p2). It is the sum of squared difference between each dimension \\((x, y, \\theta)\\) of trajectory’s final pose and our traget pose. If this value is below our threshold, then our optimization is complete. If not, we adjust \\({s, k_m, k_f}\\) parameters to try to get a better trajectory.Adjust ParametersThe trajectory’s final state can be modelled as the output of a nonlinear function \\(f\\):\\[\\begin{bmatrix}x_{f, traj} \\\\y_{f, traj} \\\\\\theta_{f, traj} \\\\\\end{bmatrix} = f(s, k_m, k_f)\\]Our parameter update step is done via the Gauss-Newton method with line search. To obtain the Jacobian we apply the centered difference approach which computes the error vector based on small perturbations to the parameters of \\(f\\)Eigen::MatrixXf TrajectoryOptimizer::calculateJacobian(Pose2D&amp; target_pose, TrajectoryParam&amp; params, float k0) {    // perturb each param, use centered diff to approx jacobian    TrajectoryParam params_pert;    Eigen::Vector3f dp, dn;    Pose2D pose_pos, pose_neg; // pose from positive and negative perturbation    // perturb s, first col of jacobian    params_pert = {params.s + delta_params_.s, params.km, params.kf};    pose_pos = generateFinalState(params_pert, k0);    dp = calculatePoseErrorVector(target_pose, pose_pos);    params_pert = {params.s - delta_params_.s, params.km, params.kf};    pose_neg = generateFinalState(params_pert, k0);    dn = calculatePoseErrorVector(target_pose, pose_neg);    Eigen::Vector3f d1 = (dp - dn) / (2.0f * delta_params_.s);    // perturb km, second col    params_pert = {params.s, params.km + delta_params_.km, params.kf};    pose_pos = generateFinalState(params_pert, k0);    dp = calculatePoseErrorVector(target_pose, pose_pos);    params_pert = {params.s, params.km - delta_params_.km, params.kf};    pose_neg = generateFinalState(params_pert, k0);    dn = calculatePoseErrorVector(target_pose, pose_neg);    Eigen::Vector3f d2 = (dp - dn) / (2.0f * delta_params_.km);    // perturb kf, third col of jacobian    params_pert = {params.s, params.km, params.kf + delta_params_.kf};    pose_pos = generateFinalState(params_pert, k0);    dp = calculatePoseErrorVector(target_pose, pose_pos);    params_pert = {params.s, params.km, params.kf - delta_params_.kf};    pose_neg = generateFinalState(params_pert, k0);    dn = calculatePoseErrorVector(target_pose, pose_neg);    Eigen::Vector3f d3 = (dp - dn) / (2.0f * delta_params_.kf);    // return matrix    Eigen::Matrix3f J;    J &lt;&lt; d1, d2, d3;    return J;}In our line search, we apply a series of predefined learning rates to our parameter vector and compute the error in the trajectory generated by the updated parameter vector. We update our parameter vector with the learning rate that generated the lowest cost.float TrajectoryOptimizer::selectLearningParam(Pose2D&amp; target_pose, TrajectoryParam params, float k0, Eigen::Vector3f&amp; dp) {    float min_cost = std::numeric_limits&lt;float&gt;::max();    float min_a = min_alpha_;    for (float a = min_alpha_; a &lt; max_alpha_; a += d_alpha_) {        TrajectoryParam new_params = params;        new_params.s += a * dp(0);        new_params.km += a * dp(1);        new_params.kf += a * dp(2);        Pose2D last_pose = generateFinalState(new_params, k0);        Eigen::Vector3f dstate = calculatePoseErrorVector(target_pose, last_pose);        float cost = dstate.norm();        if (cost &lt;= min_cost &amp;&amp; std::abs(cost) &gt; std::numeric_limits&lt;float&gt;::epsilon()) {            min_a = a;            min_cost = cost;        }    }    return min_a;}We repeat the trajectory generation-&gt;compute cost-&gt; adjust parameter cycle until we have reached a maximum number of iterations or our computer trajectory is sufficiently close to our goal."
  },
  
  {
    "title": "Robotics Blog: Model Predictive Control",
    "url": "/posts/mpc/",
    "categories": "Robotics Blog",
    "tags": "robotics",
    "date": "2020-10-16 00:00:00 +0800",
    





    "snippet": "In this post we will be going over a simple MPC Controller for 2D path tracking in a robot that uses the bicycle model.This post complements an implementation of MPC I made in my LearnRoboticsCpp repository.Source FileHeader FileModel Predictive ControlModel predictive control is common method used for path tracking in mobile robotics. MPC uses a nonlinear model of the plant and a parametrized controller to predict errors of the system within a small future window. The controller then numerically optimizes a sum of weight squared errors in the system to compute a series of control inputs for the system. This optimization process is repeatedly applied to counteract the effect of disturbances.Bicycle ModelWe denote our time-varying state \\(z_t\\) using a four dimensional vector (x, y) for position, v for velocity and \\(\\phi\\) for the yaw angle.\\[z_t = \\begin{bmatrix}x_t &amp; y_t &amp; v_t &amp; \\phi_t\\end{bmatrix} ^ T\\]The robot accepts two control inputs: a for acceleraton and \\(\\delta\\) for steering angle.\\[u_t = \\begin{bmatrix}a_t &amp; \\delta_t\\end{bmatrix} ^ T\\]The differential equations describing the robot’s motion are as follows. We omit \\(t\\) for brevity and express the first derivative of each state as\\[z' =\\begin{bmatrix}x' \\\\y' \\\\v' \\\\\\phi'\\end{bmatrix}= \\begin{bmatrix}v cos( \\phi)) \\\\v sin( \\phi ))\\\\a \\\\\\frac{ v \\tan{ \\delta } }{L}\\end{bmatrix}\\]For small timesteps, we can approximate a new state \\(z_{t+1}\\) as\\[z_{t+1} = f(z_t, u_t, \\Delta t) =\\begin{bmatrix}x_t + v_t sin(\\phi_t) \\Delta t \\\\y_t + v_t cos(\\phi_t) \\Delta t   \\\\\\phi_t + \\frac{v_t \\tan(\\delta_t) \\Delta t}{L} \\\\v_t + a_t \\Delta t\\end{bmatrix}\\]Optmization            At each time step t, we seek to find a control policy \\(U\\). $$u_{i      j}\\(denotes predicted control at time\\)j\\(for timestep\\)i$$.      \\[U = \\{ u*{t|t}, u*{t+1|t}, u\\_{t+2|t}, ... \\}\\]This control policy should minimize a cost function \\(J\\) that considers the following information: motion model of the robot, closeness to trajectory, the magnitude of controls required and the difference between controls applied at consecutive timesteps (this helps ensure passenger comfort).The mathematical expression that encodes these four factors is as follows:\\[J(z(t), U) = \\sum_{j=t}^{T} \\bar z_{j|t}^T Q \\bar z_{j|t} + \\delta z_{j|t}^T Q' \\delta z_{j|t} + u_{j|t}^T R u_{j|t} + \\delta u_{j|t}^T R' \\delta u_{j|t}\\]  \\(T\\) is the time horizon we are optimizing over. The larger this value, the more computationally expensive your optimization problem will be.  \\(\\bar z*{j|t} = z*{j+1|t} - f(z*{j|t}, u*{j|t}, \\Delta t)\\)This first term constrains the motion of the robot using the bicycle model.  \\(\\delta z*{j|t} = z*{j|t, ref} - z\\_{j|t}\\).This is the difference between the robot’s desired and current state at some time t. So the second term penalizes differences from the reference trajectory.  The third term penalizes large control inputs into the system.  \\(\\delta u*{j|t} = u*{j+1|t} - u\\_{j|t}\\).This is the difference between consecutive control inputs. The fourth term ensures a somewhat smooth control input.\\(Q, Q' \\in R^{4 x 4}\\) and \\(R,R' \\in R^{2 x 2}\\) are matrices that denote the penalty apply to each term in the cost function. For instance, a matrix with high Q values means that you want to discentivize the optimization problem from deviating from the reference trajectory too much.Additionally, our optimization problem is subject to the following contraints:      Maximum acceleration \\(a*t &lt; a*{max}\\)        Maximum steering angle \\(\\delta*t &lt; \\delta*{max}\\)        Allowable velocities \\(v_{min} &lt; v_t &lt; v_{max}\\)        First state in the optimization output should be equal to our current state \\(z_1 = z_{initial}\\)  ImplementationThis part provides implementation details regard the MPC optimization part of the code in mpcSolve function.Let us first define the variables used in our optimization. vars is a CPPAD vector that will store all the variables in the optimization. Note that the velocity and yaw was swapped in this implementation.dimension of vars = dimensions of state vector x time horizon + dimensions of control vector x (time horizon - 1)\\[vars =\\begin{bmatrix}x_1 &amp; ... &amp; x_T &amp; y_1 &amp; ... &amp; y_T &amp;  \\phi_1 &amp; ... &amp; \\phi_T &amp;v_1 &amp; ... &amp; v_T &amp;&amp; \\delta_1 &amp; ... &amp; \\delta_T &amp; a_1 &amp; ... &amp; a_T \\\\\\end{bmatrix}\\]We use the variables VAR_start (e.g. x_start, y_start, yaw_start ) to denote the start index for a variable in the vector.Dvector vars(n_vars_);for (int i = 0; i &lt; n_vars_; i++) {    vars[i] = 0.0;}vars[x_start_] = x;vars[y_start_] = y;vars[yaw_start_] = yaw;vars[v_start_] = v;vars_lowerbound and vars_upperbound will be the same size of vars and defines the lower and upperbound for each variable. Thes are the variable constraints (1), (2) and (3).// Lower and upper limits for xDvector vars_lowerbound(n_vars_);Dvector vars_upperbound(n_vars_);// Set all non-actuators upper and lowerlimits// to the max negative and positive values.// NOTE there mush be both lower and upper bounds for all vars!!!!!for (auto i = 0; i &lt; n_vars_; i++){    vars_lowerbound[i] = -10000000.0;    vars_upperbound[i] = 10000000.0;}for (auto i = delta_start_; i &lt; delta_start_ + T_ - 1; i++){    vars_lowerbound[i] = -  static_cast&lt;double&gt;(state.max_steer);    vars_upperbound[i] = static_cast&lt;double&gt;(state.max_steer);}for (auto i = a_start_; i &lt; a_start_ + T_ - 1; i++){    vars_lowerbound[i] = -static_cast&lt;double&gt;(state.max_accel);    vars_upperbound[i] = static_cast&lt;double&gt;(state.max_accel);}for (auto i = v_start_; i &lt; v_start_ + T_; i++){    vars_lowerbound[i] = static_cast&lt;double&gt;(state.min_speed);    vars_upperbound[i] = static_cast&lt;double&gt;(state.max_speed);}For IPOPT, we also need to define the function constraints of the problem. The only function constraint we need to apply is that the initial conditions stay the same.Dvector constraints_lowerbound(n_constraints_);Dvector constraints_upperbound(n_constraints_);for (auto i = 0; i &lt; n_constraints_; i++){    constraints_lowerbound[i] = 0;    constraints_upperbound[i] = 0;}constraints_lowerbound[x_start_] = x;constraints_lowerbound[y_start_] = y;constraints_lowerbound[yaw_start_] = yaw;constraints_lowerbound[v_start_] = v;constraints_upperbound[x_start_] = x;constraints_upperbound[y_start_] = y;constraints_upperbound[yaw_start_] = yaw;constraints_upperbound[v_start_] = v;We define a struct FUNC_EVAL with a functionvoid operator()(ADvector &amp;fg, const ADvector &amp;vars) which will be called by the solver.Here we add the third and fourth terms in \\(J\\) from above. We chose\\[R =\\begin{bmatrix} 0.01 &amp; 0 \\\\ 0 &amp; 0.01\\end{bmatrix}\\]\\[R' =\\begin{bmatrix} 0.01 &amp; 0 \\\\ 0 &amp; 1\\end{bmatrix}\\]fg[0] = 0;for (int i = 0; i &lt; T_ - 1; i++){    fg[0] += 0.01 * CppAD::pow(vars[a_start_ + i], 2);    fg[0] += 0.01 * CppAD::pow(vars[delta_start_ + i], 2);}for (int i = 0; i &lt; T_ - 2; i++){    fg[0] += 0.01 * CppAD::pow(vars[a_start_ + i + 1] - vars[a_start_ + i], 2);    fg[0] += 1 * CppAD::pow(vars[delta_start_ + i + 1] - vars[delta_start_ + i], 2);}Finally, we encode the first and second terms in \\(J\\). Here we chose\\[Q =\\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; 0\\\\0 &amp; 1 &amp; 0 &amp; 0 \\\\0 &amp; 0 &amp; 1 &amp; 0 \\\\0 &amp; 0 &amp; 0 &amp; 1\\end{bmatrix}\\]\\[Q' =\\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; 0\\\\0 &amp; 1 &amp; 0 &amp; 0 \\\\0 &amp; 0 &amp; 0.5 &amp; 0 \\\\0 &amp; 0 &amp; 0 &amp; 0.5\\end{bmatrix}\\]// The rest of the constraintsfor (int i = 0; i &lt; T_ - 1; i++){    // The state at time t+1 .    AD&lt;double&gt; x1 = vars[x_start_ + i + 1];    AD&lt;double&gt; y1 = vars[y_start_ + i + 1];    AD&lt;double&gt; yaw1 = vars[yaw_start_ + i + 1];    AD&lt;double&gt; v1 = vars[v_start_ + i + 1];    // The state at time t.    AD&lt;double&gt; x0 = vars[x_start_ + i];    AD&lt;double&gt; y0 = vars[y_start_ + i];    AD&lt;double&gt; yaw0 = vars[yaw_start_ + i];    AD&lt;double&gt; v0 = vars[v_start_ + i];    // Only consider the actuation at time t.    AD&lt;double&gt; delta0 = vars[delta_start_ + i];    AD&lt;double&gt; a0 = vars[a_start_ + i];    // constraint with the dynamic model    fg[2 + x_start_ + i] = x1 - (x0 + v0 * CppAD::cos(yaw0) * dt_);    fg[2 + y_start_ + i] = y1 - (y0 + v0 * CppAD::sin(yaw0) * dt_);    fg[2 + yaw_start_ + i] = yaw1 - (yaw0 + v0 * CppAD::tan(delta0) / L_ * dt_);    fg[2 + v_start_ + i] = v1 - (v0 + a0 * dt_);    // cost with the ref traj    fg[0] += CppAD::pow(xref_(0, i + 1) - (x0 + v0 * CppAD::cos(yaw0) * dt_), 2);    fg[0] += CppAD::pow(xref_(1, i + 1) - (y0 + v0 * CppAD::sin(yaw0) * dt_), 2);    fg[0] += 0.5 * CppAD::pow(xref_(2, i + 1) - (yaw0 + v0 * CppAD::tan(delta0) / L_ * dt_), 2);    fg[0] += 0.5 * CppAD::pow(xref_(3, i + 1) - (v0 + a0 * dt_), 2);}With all the required variables defined we can can now ask IPOPT to solve our equation.// place to return solutionCppAD::ipopt::solve_result&lt;Dvector&gt; solution;CppAD::ipopt::solve&lt;Dvector, FUNC_EVAL&gt;(    options, vars, vars_lowerbound, vars_upperbound, constraints_lowerbound,    constraints_upperbound, func_eval, solution);"
  },
  
  {
    "title": "Robotics Blog: Cubic Splines",
    "url": "/posts/cubic-splines/",
    "categories": "Robotics Blog",
    "tags": "robotics",
    "date": "2020-10-16 00:00:00 +0800",
    





    "snippet": "Cubic SplinesA cubic spline is constructed from a series of piecewise third degree polynomials. Given \\(n\\) control points\\(\\{(x_0,y_0), ... , (x_{n-1}, y_{n-1}) \\}\\)We can construct \\(n-1\\) segments where the $i$th segment is computed as:\\[f_i (x) = a_i + b_i (x - x_i) + c_i (x - x_i)^2 + d_i (x - x_i)^3\\]with \\(x \\in [x_i, x_{i+1}]\\)Typically we set the 2nd derivative of each \\(f_i\\) to 0 at the endpoints to create a system of \\(n-2\\) equations that we can solve for \\(Ax=b\\).\\[A =\\begin{bmatrix}1 &amp;  &amp;  \\\\  &amp; 2(h_0 + h_1) &amp; h_1 \\\\  &amp; h_1 &amp; 2 (h_1 + h_2) &amp; h_2 \\\\  &amp;     &amp;   h_2 &amp;  2(h_2 + h_3) &amp; h_3 \\\\  &amp;     &amp;         &amp; \\ddots &amp; \\ddots &amp; \\ddots &amp;  \\\\  &amp;     &amp;         &amp;     &amp; &amp;    &amp;  h_{n-3}      &amp;    2 (h_{n-3} + h_{n-2})   &amp; &amp; 0 \\\\  &amp;     &amp;         &amp;    &amp; &amp;     &amp;        &amp;   0    &amp; &amp; 1\\end{bmatrix}\\]\\[x =\\begin{bmatrix}c_0 \\\\c_1 \\\\\\vdots \\\\c_{n-1}\\end{bmatrix}\\]\\[b =\\begin{bmatrix}0 \\\\\\frac{3 (y_2 - y_1)}{h_1} - \\frac{3 (y_1 - y_0)}{h_0} \\\\\\vdots \\\\\\frac{3 (y_{n-1} - y_{n-2})}{h_{n-2}} - \\frac{3 (y_{n-2} - y_{n-3})}{h_{n-3}} \\\\0\\end{bmatrix}\\]Having solved for all \\(c_i\\)s solve for \\(d_i\\) and \\(b_i\\) using:\\[b*i = \\frac{(y*{i+1} - y*{i})}{h_i} - \\frac{1}{3} (2c_i + c*{i+1}) h_i\\]\\[d*i = \\frac{c*{i+1} - c\\_{i}}{3 h_i}\\]DerivationWe seek to solve for \\(a_i\\), \\(b_i\\), \\(c_i\\) and \\(d_i\\) for each segment of the spline with \\(i = 0, ..., n-1\\).Since all \\((x-x_i)\\) terms go zero when \\(x=x_i\\), we can easily get \\(a_i\\) with \\(f_i(x_i) = a_i = y_i\\).We require our function function to be continuous and twice differentiable. The zeroth, first and second derivative with respect to x for each segment is reproduced below.\\[f_i (x) = a_i + b_i (x - x_i) + c_i (x - x_i)^2 + d_i (x - x_i)^3\\]\\[f_i' (x) = b_i+ 2 c_i (x - x_i) + 3 d_i (x - x_i) ^ 2\\]\\[f_i'' (x) = 2 c_i + 6 d_i (x - x_i)\\]Let us define \\(h_i = x_{i+1} - x_i\\) and substitute \\(a_i = y_i\\) from here on.  Continuous zeroth derivative means \\(f*i(x*{i+1}) = y\\_{i+1}\\). For \\(i = 0, ..., n-1\\).\\[y*i + b_i (h_i) + c_i (h_i)^2 + d_i (h_i) ^3 = y*{i+1}\\]  Continuous first derivative means \\(f_{i}'(x_{i+1}) = f'_{i+1}(x_{i+1})\\). For \\(i = 0, ..., n-2\\).\\[b*i+ 2 c_i (h_i) + 3 d_i (h_i) ^ 2 = b*{i+1}\\]  Continuous second derivative means \\(f_{i}^{''}(x_{i+1}) = f_{i+1}^{''}(x_{i+1})\\). For \\(i = 0, ..., n-2\\).\\[2 c*i + 6 d_i (h_i) = 2 c*{i+1}\\]From the three equations above, we seek to express \\(d_i\\) and \\(b_i\\) in terms of \\(c_i\\).Lets start off with expressing $d_i$ in terms of \\(c_i\\) using \\(f_{i}^{''}(x_{i+1}) = f_{i+1}^{''} (x_{i+1})\\)\\[2 c*i + 6 d_i (h_i) = 2 c*{i+1}\\]\\[d*i = \\frac{c*{i+1} - c\\_{i}}{3 h_i}\\]We can also express \\(b_i\\) in terms of \\(c_i\\) using \\(f*i(x*{i+1}) = y\\_{i+1}\\).\\[y*i + b_i (h_i) + c_i (h_i)^2 + d_i (h_i) ^3 = y*{i+1}\\]\\[b*i (h_i) = (y*{i+1} - y\\_{i}) - c_i (h_i)^2 - d_i (h_i) ^3\\]\\[b*i = \\frac{(y*{i+1} - y\\_{i})}{h_i}- c_i h_i - d_i (h_i) ^2\\]Sub in \\(d_i\\).\\[b*i = \\frac{(y*{i+1} - y*{i})}{h_i}- c_i h_i - \\frac{c*{i+1} - c_i}{3 h_i} (h_i) ^2\\]\\[b*i = \\frac{(y*{i+1} - y*{i})}{h_i}- c_i h_i - \\frac{c*{i+1} - c_i}{3 } (h_i)\\]\\[b*i = \\frac{(y*{i+1} - y*{i})}{h_i} - \\frac{c*{i+1} h_i}{3} - \\frac{2 c_i h_i}{3}\\]\\[b*i = \\frac{(y*{i+1} - y*{i})}{h_i} - \\frac{1}{3} (2c_i + c*{i+1}) h_i\\]Subbing our expressions for \\(b_i\\) and \\(d_i\\) into the equation formed from \\(f_{i}'(x_{i+1}) = f'_{i+1}(x_{i+1})\\).\\[b*i+ 2 c_i (h_i) + 3 d_i (h_i) ^ 2 = b*{i+1}\\]\\[[\\frac{(y_{i+1} - y_{i})}{h_i} - \\frac{1}{3} (2c_i + c_{i+1}) h_i] + 2 c*i h_i + 3[ \\frac{c*{i+1} - c*{i}}{3 h_i}] h_i^2 = \\frac{(y*{i+2} - y*{i+1})}{h*{i+1}} - \\frac{1}{3} (2c*{i+1} + c*{i+2}) h\\_{i+1}\\]\\[\\vdots\\]\\[h*i c_i + 2(h_i + h*{i+1}) c*{i+1} + h*{i+1} c*{i+2} = 3 \\frac{y*{i+2} - y*{i+1} }{h*{i+1}} - 3 \\frac{y*{i+1} - y*{i} }{h_i}\\]Because we set the second derivative to zero at the end points of the spline, we can set \\(c_0 = c_{n-1} = 0\\). So we only have to solve for \\(n-2\\) \\(c_i\\)’s. Conveniently, we have \\(n-2\\) “versions” of the equation above with \\(i = 0, ..., n-2\\). This allows us to create a system of linear equations of the form \\(Ax=b\\) where we can solve for all \\(c_i\\)’s. The top left and bottom right 1s in the \\(A\\) matrix encode \\(c_0 = c_{n-1} = 0\\), the rest of the matrix expresses encodes the \\(n-2\\) equations for \\(c_i\\)\\[A =\\begin{bmatrix}1 &amp;  &amp;  \\\\  &amp; 2(h_0 + h_1) &amp; h_1 \\\\  &amp; h_1 &amp; 2 (h_1 + h_2) &amp; h_2 \\\\  &amp;     &amp;   h_2 &amp;  2(h_2 + h_3) &amp; h_3 \\\\  &amp;     &amp;         &amp; \\ddots &amp; \\ddots &amp; \\ddots &amp;  \\\\  &amp;     &amp;         &amp;     &amp; &amp;    &amp;  h_{n-3}      &amp;    2 (h_{n-3} + h_{n-2})   &amp; &amp; 0 \\\\  &amp;     &amp;         &amp;    &amp; &amp;     &amp;        &amp;   0    &amp; &amp; 1\\end{bmatrix}\\]\\[x =\\begin{bmatrix}c_0 \\\\c_1 \\\\\\vdots \\\\c_{n-1}\\end{bmatrix}\\]\\[b =\\begin{bmatrix}0 \\\\\\frac{3 (y_2 - y_1)}{h_1} - \\frac{3 (y_1 - y_0)}{h_0} \\\\\\vdots \\\\\\frac{3 (y_{n-1} - y_{n-2})}{h_{n-2}} - \\frac{3 (y_{n-2} - y_{n-3})}{h_{n-3}} \\\\0\\end{bmatrix}\\]Having solved for all \\(c_i\\)s we can use our expressions for \\(d_i\\) and \\(b_i\\) in terms of \\(c_i\\) to solve for our final coefficients."
  },
  
  {
    "title": "Robotics Blog: Quintic Polynomials",
    "url": "/posts/quintic-polynomials/",
    "categories": "Robotics Blog",
    "tags": "robotics",
    "date": "2020-10-15 00:00:00 +0800",
    





    "snippet": "IntroductionQuintic splines are a common path parametrization in robotics. A quintic polynomial is a 5th order polynomial. Each dimension \\((x, y)\\) is represented as a quintic polynomial. The coefficients \\(a_i\\) and \\(b_i\\) can be solved for provided we give the robot’s start position, velocity and acceleration (\\(x_s\\), \\(v_s\\), \\(a_s\\)) as well as the end position, velocity and acceleration (\\(x_f\\), \\(v_f\\), \\(a_f\\)); these are also known the boundary conditions.\\[x(t) = a_o + a_1 t + a_2 t ^2 + a_3 t ^ 3 + a_4 t ^ 4 + a_5 t^5\\]\\[y(t) = b_o + b_1 t + b_2 t ^2 + b_3 t ^ 3 + b_4 t ^ 4 + b_5 t^5\\]IssuesThere are discontinuities in the curvature paramatrization of quintic splines, which makes the curvature hard to constrain. From the equation of curvature \\(k(t)\\) below, we can see that the denominator of the equation could be undefined.\\[k(t) = \\frac{x'(t) y''(t) - y'(t)x''(t)}{ (x'(t)^2 + y'(t)^2)^{3/2}  }\\]Solving the EquationLet us consider only \\(x(t)\\). We can solve for \\(a_i\\) provided that we are given the start position, velocity and acceleration (\\(x_s\\), \\(v_s\\), \\(a_s\\)) as well as the end position, velocity and acceleration (\\(x_f\\), \\(v_f\\), \\(a_f\\)); these are also known the boundary conditions of \\(x(t)\\) ‘s zeroth, first and second derivative.First derivative:\\(x'(t) = a_1 + 2 a_2 t  + 3 a_3 t ^ 2 + 4 a_4 t ^ 3 + 5 a_5 t^ 4\\)Second derivative:\\(x''(t) = 2 a_2 t  +  6 a_3 t + 12 a_4 t ^ 2 + 20 a_5 t^ 3\\)Using our initial boundary conditions for $t = 0$ we can easily solve for \\(a_0\\), \\(a_1\\), \\(a_2\\).\\[x(0) = x_s = a_0\\]\\[x'(0) = v_s = a_1\\]\\[x''(0) = a_s = 2 a_2\\]Now lets solve for \\(a_3\\), \\(a_4\\), \\(a_5\\) using our boundary conditions of \\(t = T\\).Eqn 1: \\(x(T) = x_e = a_o + a_1 T + a_2 T ^2 + a_3 T ^ 3 + a_4 T ^ 4 + a_5 T^5\\)Eqn 2: \\(x'(T) = v_e = a_1 + 2 a_2 T  + 3 a_3 T ^ 2 + 4 a_4 T ^ 3 + 5 a_5 T^ 4\\)Eqn 3: \\(x''(T) = a_e = 2 a_2 T  +  6 a_3 T + 12 a_4 T ^ 2 + 20 a_5 T^ 3\\)Since we have already solved for \\(a_0\\), \\(a_1\\), \\(a_2\\), we can rearrange (1), (2) and (3) to have all of our unknwons on the left and known variables on the right side of the equality.Eqn 1: \\(a_3 T ^ 3 + a_4 T ^ 4 + a_5 T^5 = x_e - a_0 - a_1 T - a_2 T ^2\\)Eqn 2: \\(3 a_3 T ^ 2 + 4 a_4 T ^ 3 + 5 a_5 T^4 = v_e - a_1 - a_1 T - 2 a_2 T\\)Eqn 3: \\(6 a_3 T + 12 a_4 T ^ 2 + 20 a_t T^3 = a_e - 2 a_2\\)The three equations above can now be conveniently factored into a system of linear equations $Ax=b$.\\[\\begin{pmatrix}T^3 &amp; T^4 &amp; T^5 \\\\3T^2 &amp; 4 T ^3 &amp; 5 T ^ 4 \\\\6T &amp; 12 T ^2 &amp; 20 T^3\\end{pmatrix}\\begin{pmatrix}a_3 \\\\a_4 \\\\a_5\\end{pmatrix}=\\space\\begin{pmatrix}x_e - a_0 - a_1 T - a_2 T ^2 \\\\v_e - a_1 - a_1 T - 2 a_2 T \\\\a_e - 2 a_2\\end{pmatrix}\\]Since \\(T \\geq 0\\) our A matrix will be invertible and we can solve for \\(a_3\\), \\(a_4\\), \\(a_5\\) using \\(x = A^{-1} b\\)."
  },
  
  {
    "title": "Cpp Blog: Exceptions",
    "url": "/posts/exceptions/",
    "categories": "Cpp Blog",
    "tags": "cpp",
    "date": "2020-09-24 00:00:00 +0800",
    





    "snippet": "Cpp Blog: ExceptionsThis post contains my notes from Klaus Iglberger’s talk on Exceptions from CppCon 2020.Back to Basics: Exceptions - Klaus Iglberger - CppCon 2020 - YouTubeIntro to ExceptionsExceptions is a widely debated topic in the C++ community. Some companies use it and some companies completely ban the use of exceptions in their code bases. There are four main reasons why people are sometimes against using exceptions:  When an error occurs during an exception, it will incur a significant penalty.  Exceptions make it hard to reason about functions since we typically expect functions to have one set of output(s).  Exceptions rely of dynamic memory. On some embedded platforms this might not be available or desirable.  Exceptions increase the size of your binary file.Mechanics of an ExceptionAn exception is usually characterized by three keywords throw, try and catch. Below is a snippet of code that demonstrates the use of exceptions in C++ code. Your main() will try to call someFunction(), if the error conditions occurs then someFunction() will throw an exception which we will catch.void someFunction() {    std::string s = \"blah blah\";    if ( /* some error condition */) {        throw std::runtime_error(\"you messed up...\");    }    // ...}int main() {    try {        // ...        someFunction();    } catch( std::exception const&amp; ex )  {        /* handle the exception */    }}In the case of an exception, a stack unwinding will occur in which objects on the stack will be destroyed in reverse order. So in this case, when std::runtime_error is thrown, we will destroy s then any stack initializations that happen before someFunction() in the try block.If an exception is not caught, no stack unwinding will occur and std::terminate will be called. This could potentially lead to leaked resources.When to use exceptionsThere are three scenarios in which you would want to use an exception:  For errors that occur very rarely.  For errors that cannot be dealt with locally. For instance, if a file is not found in a function or a key is not in the map.  For operators and constructors. These functions do not have any other mechanisms for error feedback.When not to use exceptions  For errors that are expected to fail frequently.  For functions that are expected to fail sometimes (e.g converting string to int). Use std::optional or boost::expected in these cases.  If you require guaranteed response times.  Things that should never happen (e.g. dereference nullptrs or out-of-range access). Use assert in these scenarios.Tips on using exceptionsIt is best practice to build on the standard std::execption when using exceptions. You should try to throw by rvalue to efficiency reasons. Also, it is best to catch by const references to avoid unnecessary copies.Exception Safety GuaranteesIglberger identifies 4 levels of exception safety guarantees.  exception unsafe          Mo guarantees with respect to invariants and resources        basic exception safety gurantees          invariants are preserved      no resources are leaked        strong exception guarantees          invariants are preserved      no resources are leaked      no state change      not always possible        no-throw guarantee          the operation cannot fail      expressed in code with noexcept      Striving for a no-throw guarantee is always the best option. Below is an example of a class that follows this guarantee.class Widget {private:    int i = 0;    std::string s;    std::unique_ptr&lt;Resource&gt; ptr;public:    // copy constructor    Widget( Widget const&amp; w) :        i { w.i },        s { w.s }    {        if (w.ptr)            ptr = std::make_unqiue&lt;Resource&gt; ( *w.ptr );    }    // Copy assignment operator    Widget&amp; operator=(Widget const&amp; w) {        if (this === &amp;w)            return *this;        Widget tmp(w); // temp-move idiom        *this = std::move(tmp);        return *this;    }    Widget&amp; operator=( Widget&amp;&amp; w) noexcept;}"
  },
  
  {
    "title": "Cpp Blog: Atomics, Locks and Tasks",
    "url": "/posts/atomics-locks-tasks/",
    "categories": "Cpp Blog",
    "tags": "cpp",
    "date": "2020-09-23 00:00:00 +0800",
    





    "snippet": "Cpp Blog: Atomics, Locks and TasksThese are my notes from CppCon from Rainer Grimm’s talk titled “Atomics, Locks and Tasks”.DefinitionsA data race occurs when two threads access shared state concurrently and at least one thread tries to modify the shared state. This could lead to undefined behaviour.A critical section is a shared state which can only be accessed concurrently by at most one thread.A deadlock is a state in which one thread is blocked forever because it is waiting for the release of a resource it can never acquire.Mutexstd::mutex is a conccurrency primitive that ensures only one thread can enter a critical section at once. It supports several locking related methods such as lock(), unlock() and try_lock().Typically std::mutex is wrapped with std::unique_lock or std::lock_guard which are RAII (resource acquisition is initialization) wrappers. Both wrappers will call lock() on a given mutex on construction and call unlock() on the given mutex on destruction. A unique_lock offers a richer set of functionalities over a lock_guard. For instance, unique_lock allows the mutex to be repeatedly locked and unlocked.#include &lt;mutex&gt;std::mutex m;// using lock_guard{    std::lock_guard&lt;std::mutex&gt; lock(m1);    // critical section}// using unique_lock{    std::unique_lock&lt;std::mutex&gt; lock(m1);    // critical section}AtomicsAtomic variables allows threads to concurrently access and update variables without data races. These synchronizations are typicaly achieved with hardware. std::atomics are typically used for primitive types such as int, bool, float, etc.Here is an example of multiple threads concurrently updating an atomic variableauto constexpr num = 100;std::vector&lt;std::thead&gt; vec(num);std::atomic&lt;int&gt; i;// create num threads that each update i 10 timesfor (auto &amp;t : vec) {    t = std::thread([&amp;i]{        for (int n = 0; n &lt; 10; ++n) ++i;    })}// join our threadsfor (auto &amp;t : vec)    t.join();Acquiring Multiple Locks at the Same TimeSuppose Jim and Kate were sitting at the dinner table and there was only one pair of chopsticks. If each of them have a chopstick, one of them will have to yield their chopsticks otherwise no one gets to eat. It is best if one person acquires both sticks at the same time.Similarly, threads should acquire mutexes of interest at the same time otherwise we could have a deadlock. std::lock accepts an arbitruary number of mutexes that it will attempt to lock at the same time.std::unique_lock&lt;std::mutex&gt;(mutex1, std::defer_lock);std::unique_lock&lt;std::mutex&gt;(mutex2, std::defer_lock);std::lock(mutex1, mutex2); // can lock arbitrary of unique locks in an atomic fashionIn C++17, we can use std::scoped_lock which serves the exact function as std::lock except you do not need to declare the unique_locks before the function call.std::scoped_lock&lt;std::mutex&gt; scoped_lock(mutex1, mutex2);Scoped Thread and jthreadWhen we spawn an std::thread we must always remember to call .join() or .detach() on it. Otherwise, our program will throw an error. Usually we will write RAII wrappers around our threads to automatically do this for us. Here is an example:class ScopedThread {private:    std::thread t_;public:    explicit ScopedThread(std::thread t) : t_(std::move(t)) {        if (t.joinable() == false)            throw std::logic_error(\"No thread\");    }    ~ScopedThread() {        t.join();    }    // delete copy and copy assignment operator    // we don't wanna end up with multiple instances of    // ScopedThread calling join() on the same thread    ScopedThread(ScopedThread&amp;) = delete;    ScopedThread&amp; operator=(ScopedThread const &amp;) = delete;}TasksGrimm describes tasks in C++ as a data channel where you have a sender with an std::promise and a receiver using std::future.Tasks vs ThreadsHere is a small code example comparing threads with tasks in C++.int result;std::thread t([&amp;]{ result = 3 + 4; });t.join();std::cout &lt;&lt; res &lt;&lt; std:::endl;// std::async is a promiseauto fut = std::async([]{ return 3 + 4; });std::cout &lt;&lt; fut.get() &lt;&lt; std:::endl;Grimm shared a wonderful table in his presentation outlining the difference between tasks and threads in C++.| Characteristic | Thread | Task ||—————————|————————————-|————————————–|| header | &lt;thread&gt; | &lt;future&gt; || participants | creator and child thread | promise and future || communication | shared variable | communication channel || synchronization | join call waits | get call blocks || exception in child thread | child and creator thread terminates | return value of the get call || kind of communication | values | values, notifications and exceptions |Parallel STLSince C++17 the standard library has offered many parallelized versions of common algorithms such as sort, count, transform etc. The STL offers three execution policies for these algorithms:  std::execution::seq = do sequentially in one thread  std::execution::par = parallel execution  std::execution::par_unseq = parallel and vectorized (uses SIMD for even greater speedup)Note that (3) might not necessarily offer any speedup depending on your hardware and what version of compiler you’re using.Condition VariablesCondition variables (cv) enable you to synchronize threads. You can have a sender thread send notifications using cv.notify_one() or cv.notify_all(). You can have you receiver thread wait for notifications usingcv.wait(lock, ...), cv..wait_for(lock, relTime, ...) or cv.wait_until(lock, absTime, ...). The ... is used for a boolean condition to tackle the lost wakeup and spurious wakeup problem. The lost wakeup happens when the sender thread notifies the receiver thread before it begins waiting so the wakeup is lost forever. The spurious wakeup happens when a receiving thread wakes up, only to find that the condition it was waiting for is not satisfied yet.Here is a simple use case for condition variablesstd::condition_variable cv;// sender{  std::lock_guard&lt;std::mutex&gt; lck(mut); // use lock guard here  read = true;}cv.notify_one();// receiver{  std::unique_lock&lt;std::mutex&gt; lck(mut);  cv.wait(lck, []{ return ready;}); // ready is a predicate  // do the work}Typically promises and futures are preferred over conditional variables. However promises only enable monodirectional thread synchronization where as conditional variables enable bidirectional thread synchronization."
  },
  
  {
    "title": "Cpp Blog: Move Semantics Part 2",
    "url": "/posts/move-semantics-2/",
    "categories": "Cpp Blog",
    "tags": "cpp",
    "date": "2020-09-20 00:00:00 +0800",
    





    "snippet": "Forwarding ReferencesA forwarding reference is like a shapeshifter. It is an lvalue if initialized by an lvalue and rvalue if initialized by an rvalue. They typically appear in the form of T&amp;&amp; or auto&amp;&amp; and are used when type deduction is involved.template &lt;typename T&gt;void f(T&amp;&amp; x);auto&amp;&amp; var2 = var1;Reference CollapsingThe rules below are used by the compiler to collapse lvalue and rvalue references adjacent to one another.&amp; &amp; -&gt; &amp;&amp;&amp; &amp; -&gt; &amp;&amp; &amp;&amp; -&gt; &amp;&amp;&amp; &amp;&amp; -&gt; &amp;&amp;Perfect forwardingstd::forward will considtionally cast its input into an rvalue reference. Given an lvalue, it will cast the input to lvalue. Given an rvalue, it will cast the input to an rvaluetemplate &lt;typename T&gt;T&amp;&amp; forward(std::remove_reference_t&lt;T&gt;&amp; t) noexcept {    return static_cast&lt;T&amp;&amp;&gt;(t);}Suppose I pass an lvalue, so we replace T with SomeClass&amp;. By reference collapsing SomeClass&amp; &amp;&amp; become SomeClass&amp;. Remove reference will strip lvalue or value references from the SomeClass type so std::remove_reference_t&lt;SomeClass&amp;&gt;&amp; t evaluates to SomeClass&amp; t. The resulting forward function instantiated is.template &lt; &gt;SomeClass&amp; forward(SomeClass&amp; t) noexcept {    return static_cast&lt;SomeClass&amp;&gt;(t);}Suppose I pass an rvalue, so we replace T with SomeClass&amp;&amp;. Similar to the logic above, SomeClass&amp;&amp; &amp;&amp; become SomeClass&amp;&amp; and std::remove_reference_t&lt;SomeClass&amp;&amp;&gt;&amp; t evaluates to SomeClass&amp; t.The resulting forward function instantiated is.template &lt; &gt;SomeClass&amp;&amp; forward(SomeClass&amp; t) noexcept {    return static_cast&lt;SomeClass&amp;&amp;&gt;(t);}Perfect ForwardingWith the help of perfect forwarding we can forward the type of an argument into a function into another function being called. To illustrate the power of perfect forwarding we use the example of the make_unique function. A naive approach might be similar to the ones below.// the argument `arg` is always copied into function and passed as lvalue to Ttemplate&lt;typename T, typename Arg&gt;unique_ptr&lt;T&gt; make_unique(Arg arg) {    return unique_ptr&lt;T&gt;(new T(arg) );}// `arg` is passed as lvalue to T, but what if I want an rvalue to be passed to T :(template&lt;typename T, typename Arg&gt;unique_ptr&lt;T&gt; make_unique(Arg&amp;&amp; arg) {    return unique_ptr&lt;T&gt;(new T(arg) );}With std::forward we can forward the type of args into T’s constructor. So T can now be initialized using an rvalue or lvalue without the need for expensive copies.template&lt;typename T, typename... Args&gt;unique_ptr&lt;T&gt; make_unique(Args&amp;&amp;... args) {    return unique_ptr&lt;T&gt;(new T(std::forward&lt;Args&gt;(args)... ) );}MoveThe machanics of std::move is very similar to std::foward in that it uses std::remove_reference_t to gernerate the correct reference type.template &lt;typename T&gt;std::remove_reference_t&lt;T&gt;&amp;&amp; move(T&amp;&amp; t) noexcept {   return static_cast&lt;std::remove_reference_t&lt;T&gt;&amp;&amp;&gt;(t);}"
  },
  
  {
    "title": "Cpp Blog: Move Semantics Part 1",
    "url": "/posts/move-semantics/",
    "categories": "Cpp Blog",
    "tags": "cpp",
    "date": "2020-09-19 00:00:00 +0800",
    





    "snippet": "This is a video summary of Klaus Iglberger talk at CppCon 2019 titled “Back to Basics: Move Semantics (part 1 of 2)”.What does it mean to move something?Moving an object in C++ allows us to avoid a lot of wasteful copying in our projects.Consider the function called below.std::vector&lt;int&gt; createVector() {    return {1, 2, 3};}std::vector&lt;int&gt; v2;v2 = createVector();Within the scope of createVector() a temporary object is created. On the assignment to v2 we do not want to create a deep copy of the temporary object since it will be destroyed once the function goes out of scope. Move allows us to efficiently transfer data from the temporary vector to v2. In Klaus talk he simplifies a vector as having three fields: begin pointer, end pointer and pointer for end of memory allocated to vector. Our move operation will instead copy the three aforementioned pointers into v2 from the temporary object and set the three pointers in the temporary object to null.Lvalue vs. RvalueAn lvalue is an object that occupies an identifiable location in memory. An rvalue is an object that does not represent an identifiable locaiton in memory.int a = 2; // a is lval, 2 is rvalstd::string b = \"blah\"; // b is lval, b is rvalauto res = createVector(); // res is lval, object created in function is rvalFunctions can take an lvalue or an rvalue as input parameter. &amp; for lvalue, &amp;&amp; for rvalue.void someFunction(vector&amp; vec); // 1. takes an lvaluevoid someFunction(vector&amp;&amp; vec); // 2. takes an rvaluevector&lt;int&gt; v1 = {1, 2};someFunction(v1); // calls (1)someFunction({1, 2, 3}); // calls (2)Move Constructor and Move Assignment OperatorSince C++11, classes have two additional member functions: move constructor and move operator.class SomeClass {private:    int i;    std::string str;    int *ptr = nullptr;public:    SomeClass(SomeClass&amp;&amp; s); // move constructor    SomeClass&amp; operator=(SomeClass&amp;&amp; s); // move operator};The goal of the move constructor is to transfer the content of s into the current object and leave s in a valid but undefined state. In the code below we define a simple move constructor for SomeClass. We see that the move constructor simply calls move on all field of s to achieve the transfer of contents. Before we terminate, we set s.ptr to nullptr to ensure that s is left in a valid state. The C++ core guidelines also advises us to declare move constructors as noexcept because none of the operations in our move constructor should throw exceptions. According to Klaus’ experiments, using no except provided a 60% speedup.SomeClass::SomeClass(SomeClass&amp;&amp; s) noexcept :    i ( std::move(s.i) ),    str ( std::move(s.str) ),    ptr ( std::move(s.ptr) ){    s.ptr = nullptr;}The goal of the move assignment operator is to clean of all visible resources, transfer the contents of s and leave s in a valid but undefined state. delete handles the deletion of cleaning up of resources. Once again, we move every single field of s into our current object. Finally, we leave s in a defined state by setting its ptr field to nullptr.SomeClass::SomeClass&amp; operator=(SomeClass&amp;&amp; s){    delete ptr;    i = std::move(s.i);    str = std::move(s.str);    ptr = std::move(s.ptr);    s.ptr = nullptr;    return *this;}When will the compiler generate move operations (move constructor and move operator)?If no copy operation (constructor or assignment operator) or destructor is user-defined then the compiler will NOT generate move operations.If no move operation are user-defined, copy operations WILL be generated by the compiler.One thing to note is that definition member functions with =default or =delete are considered default operations.When to define your move operations?In general try to avoid defining any move operations, copy operations and destructors yourself (rule of zero). But there are times when resource management needs to be user defined. The default move operations will simply call move on every single field of the object. Because we used a raw pointer in SomeClass above, we have to cleanup its resources with delete ptr."
  },
  
  {
    "title": "Cpp Blog: Return Value Optimization & Named Return Value Optimization",
    "url": "/posts/return-value-optimization/",
    "categories": "Cpp Blog",
    "tags": "cpp",
    "date": "2020-09-18 00:00:00 +0800",
    





    "snippet": "Return Value Optimization (NRVO) &amp; Named Return Value Optimization (NRVO)Cope samples borrowed from Shahar Mike’s Web Spot.What is this?NVO and NRVO are two types of return value optmization that the C++ compiler performs for us.Consider a call:T some_variable = functionName();The idea is for the compiler to use the memory space for some_variable outside of functionName()’s scope to directly initialize the object return from the function.Without RVO or NRVO, the compiler would be creating duplicate copies of the same object. To illustrate this point, consider this definition of SomeClass which just couts the type of constructor/operator/destructor being called on it.class SomeClass{public:    SomeClass()    {        std::cout &lt;&lt; \"Constructor called\" &lt;&lt; std::endl;    }    ~SomeClass()    {        std::cout &lt;&lt; \"Destructor called\" &lt;&lt; std::endl;    }    SomeClass(const SomeClass &amp;s)    {        std::cout &lt;&lt; \"Copy constructor called\" &lt;&lt; std::endl;    }    SomeClass(SomeClass &amp;&amp;s)    {        std::cout &lt;&lt; \"Move constructor called\" &lt;&lt; std::endl;    }    SomeClass &amp;operator=(const SomeClass &amp;s)    {        std::cout &lt;&lt; \"Copy assignment called\" &lt;&lt; std::endl;        return *this;    }    SomeClass &amp;operator=(SomeClass &amp;&amp;s)    {        std::cout &lt;&lt; \"Move assignment called\" &lt;&lt; std::endl;        return *this;    }};Suppose we have some factory function for SomeClass that our code calls on:SomeClass createSomeClassObject() {    return SomeClass();}int main() {    auto obj = createSomeClassObject();}We will run the aforementioned code twice. Once with the -fno-elide-constructors argument which disables RVO and once with RVO enabled.$ clang++ -std=c++11 -fno-elide-constructors main.cpp &amp;&amp; ./a.outConstructor calledMove construct calledDestructor calledMove constructor calledDestructor calledDestructor called$ clang++ -std=c++11 main.cpp &amp;&amp; ./a.outConstructor calledDestructor calledWith RVO we can see that SomeClass is only constructed once. On the other hand, the compiler creates 3 instances of SomeClass. The first instance is created when we call return SomeClass in createSomeClassObject(). The second instance is created for the returned object inside main(). The third instance is created for the named object obj.NRVO operates very similar to RVO, except the return object is named within the function.// RVOSomeClass createSomeClassObject() {    return SomeClass();}// NRVOSomeClass createSomeClassObject() {    auto some_class = SomeClass()    return some_class;}When RVO doesn’t happen?  Function returns an instance that is determined at runtime.  Function returns a parameter to the function or a global variable.  Function returns by std::move()  Using operator= on an existing object.  Return member variables.Sources  Return Value Optimization | Shahar Mike’s Web Spot  Return value optimizations - Fluent C++"
  },
  
  {
    "title": "Cpp Blog:  Curiously Recurring C++ Bugs at Facebook",
    "url": "/posts/curriously-recurring-bugs-at-facebook/",
    "categories": "Cpp Blog",
    "tags": "cpp",
    "date": "2020-09-17 00:00:00 +0800",
    





    "snippet": "This is my summary of a CppCon talk by Louis Brandy on the “Curiously Recurring C++ Bugs at Facebook”.Bug 1: std::vector::operator[], array[], *ptrIt is quite common for us to check the index of an element outside of a vector’s range.GCC and Clang both have a -fsanitize=address option helps you check for errors like these. Typically, you can build your unit tests with this option.Bug 2: std::map::operator[]In C++ square bracket operator will actually create an element if the element you are trying to find does not exist. If the value in the map is an int, it will be initialized to 0.map&lt;string, int&gt; mp;mp[\"hey\"] = 12;std::cout &lt;&lt; mp[\"hye\"]; // THIS PRINTS: 0Brandy does not have many suggestions for this bug other than maybe using the at() operator in std::map which will throw if the key is not in the map.Bug 3: Smuggling a reference to a temporary through a functionA common appearance of smuggling references occurs when people create a function that tries to get a value from a map using a given key that returns a default value if not found.// INCORRECTauto &amp;v = get_or_return_default(my_map, key, \"123\");This code is incorrect. If key is not in my_map, v is a reference to a temporary that has now been destroyed.Brandy’s method of dealing with this bug is using the -fsanitize-address-use-after-scope flag to identify these issues.Bug 4: VolatileSome programmers mistakenly believe that volatile will make their code threadsafe and thus use misuse the keyword.To address this, Brandy added a lint rule for all usecases of volatile.Bug 5: std::shared_ptrSome programmers mistakenly believe that std::shared_ptr is threadsafe. Only the control block of std::shared_ptr is threadsafe, not the actual pointer.Brandy mitigates these issues by using thread sanitizers and using atomic shared pointer implementations.Another common issue is when programmers try to dereference a shared pointer immediately after it is returned from a function.auto&amp; ref = *returns_a_shared_ptr();ref.do_something(); // whoops, ref might be emptyAfter you dereference the shared pointer above, the shared pointer is gone so you will lose the protection count of a shared pointer. Luckily, sanitizers can catch this problem too.Bug 6: Not lockingSometimes when programmers declare a unique_lock they will forget to add a variable name to it. As such, the lock is never initialized and the code is not threadsafe.unique_lock&lt;mutex&gt; g(m_mutex); // works fineunique_lock&lt;mutex&gt; (m_mutex); // oops I forgot the gBrandy’s method of recommended way of mitigating this is using the -Wshadow to find this. He saids that Facebook has programmed their linters to find this.ConclusionsThe lesson from this talk is that tools are your best weapon. Enabling certain compiler flags for address and thread santizers can help you prevent many unwanted bugs."
  },
  
  {
    "title": "Cpp Blog: Header-only vs. Static vs. Dynamic Libraries",
    "url": "/posts/header-static-dynamic-libraries/",
    "categories": "Cpp Blog",
    "tags": "",
    "date": "2020-09-13 00:00:00 +0800",
    





    "snippet": "Header-only vs. Static vs. Dynamic LibrariesLibraries are collections of reusable code for others to use. Static and dynamic libraries will expose header files that define the functionality of the library in addition to pre-compiled binaries that contain the implementation of the library. Header-only libraries define and implement their library’s functionality in its header file(s).Header-onlyMany popular C++ libraries are implemented as header-only. For instance, Catch2 a popular unit testing framework and nlohmann-json a popular json manipulating framework.Advantages:  This is the only option when you are writing a library using templates.  Clients of the library do not need to compile it beforehand. The header files just need to be added to an existing C++ project.Disadvantages:  Compilation times are much greater because the clients of the library will have to compile their code along with your library.  Your compilation becomes more entangled. When changes are made to the header-only library, the client will have to recompile all of their code.  Since you cannot create pre-compiled binaries as in static and dynamic libraries, you cannot hide any part of your implementation.Static &amp; DynamicStatic libraries will have a .lib (Windows) or a .a signature (Linux). These routines will be compiled and linked directly with the client’s code.Dynamic libraries will have a .dll (Windows) or a .so signature (Linux). These routines will be loaded with the client’s application at runtime.Googletest is an unit testing library that can be built as a static of dynamic library.We compare the static vs. dynamic libraries in the table below                   Static      Dynamic      Explanation                  Avoiding versioning issues      v             Static libraries are compiled with the project source code so versioning issues will be discovered at compile time.              Reusability across projects             v      Static libraries have to be recompiled for every project that that uses them. Dynamic libraries do not.              Ease of updating             v      All projects using a static library need to be recompiled ob library update. Dynamic ones only update the library files.              Speed      v             Static libraries have a slight speed advantage given that they are compiled with the client code.              Code size             v      Static libraries will incease the size of code in the project binaries.      Sources  https://stackoverflow.com/questions/140061/when-to-use-dynamic-vs-static-libraries  https://stackoverflow.com/questions/12671383/benefits-of-header-only-libraries#:~:text=5%20Answers&amp;text=There%20are%20situations%20when%20a,the%20library%20might%20be%20used.  https://www.learncpp.com/cpp-tutorial/a1-static-and-dynamic-libraries/  https://medium.com/swlh/linux-basics-static-libraries-vs-dynamic-libraries-a7bcf8157779"
  },
  
  {
    "title": "Cpp Blog: Rule of 5, 3, 0 in C++",
    "url": "/posts/rule-of-5-3-0/",
    "categories": "Cpp Blog",
    "tags": "cpp, idiom",
    "date": "2020-09-12 00:00:00 +0800",
    





    "snippet": "Rule of 5, 3, 0In C++ there are six functions that pertain to the lifecycle management of an object: constructor, copy constructor, copy assignment operator, move constructor, move assignment operator and the destructor. For any of the aforementioned functions, the compiler will generate an implementation for you if you do not declare or delete them. The snippet below demonstrates each of these operators.class MyClass {public:  // constructor  // calls default constructor of each class member and of base class  MyClass() = default;  // copy constructor  // calls copy constructor of each class member and of base class  MyClass(MyClass const&amp; other) = default;  // copy assignment  // calls copy assignment of each class member and of base class  MyClass&amp; operator=(MyClass const&amp; other) = default;  // move constructor  // calls move constructor of each class member and base class  MyClass(MyClass&amp;&amp; other) = default  // move assignment  // calls move assignment of each class member and base class  MyClass&amp; operator=(MyClass&amp;&amp; other) = default;  // destructor  // calls the destructor of each class member and base class  ~MyClass() = default;};// Equivalentlyclass MyClass{};There are three important rules surrounding when and how we should use these functions:  Rule of Three  Rule of Five  Rule of ZeroRule of Three  The Rule of Three states that: if your class requires a user-defined destructor, copy constructor or a copy assignment operator, then you should define all three functions.  The aforementioned functions are user-defined when a class is managing a resource of non-object type (e.g. raw pointer or file descriptor) so the author will have to instruct the compiler on how to allocate/deallocate the resource.Rule of Five  The Rule of Five states that: if you define a move constructor or move assignment operator, then you should define both functions in addition to the functions involved in the Rule of Three (destructor, copy constructor and copy assignment operator).  The Rule of Five is a modern version of the Rule of Three since it was only in C++11 that the move semantics was introduced.Rule of Zero  The Rule of Zero states that: if your class does not require a copy/move constructor, a copy/move assignment operator, or a custom destructor then do not define them. This is often the case for classes where all its members have the five aforementioned functions defined.  Generally speaking, classes that deal with ownership will have the aforementioned five functions defined. Otherwise, they should not be defined."
  },
  
  {
    "title": "Cpp Blog: Pimpl Idiom C++",
    "url": "/posts/pimpl-idiom/",
    "categories": "Cpp Blog",
    "tags": "cpp, idiom",
    "date": "2020-09-11 00:00:00 +0800",
    





    "snippet": "PIMPLWhat is PIMPL?The PIMPL C++ idiom is typically used to reduced compile-time dependencies and construct C++ library interfaces with a stable ABI (application binary interface). This technique involves moving the implementation details of a class from it’s object representation into a separate class accessed through an opaque pointer.Below is a sample implementation of the PIMPL idiom in C++.Header Fileclass VisibleClass {  VisibleClass();  ~VisibleClass();  // ...private:  class HiddenImpl; // things to hide  unique_ptr&lt;HiddenImpl&gt; pimpl_; // opaque ptr};Implementation Fileclass VisibleClass::HiddenImpl {  // ...};VisibleClass::VisibleClass() : pimpl_(new HiddenImpl) {}VisibleClass::~VisibleClass() = default;In C++, deleting a pointer in which its type is not defined (only forward declared) will lead to undefined behaviour. To prevent this, unique_ptr will check if the definition of the type its pointer to is visible before calling delete in its destructor. If we were to not declare the destructor of VisibleClass, the compiler would generate an inlined version of it for us in the header file. Since the type of HiddenImpl is incomplete in the header file, your code will not compile. The solution is to define your own destructor so that by the time unique_ptr tries to deallocate the resources for HiddenImpl in Line 6 of the implementation file, class VisibleClass::HiddenImpl; will be fully defined.It is recommended that you place all private nonvirtual members into the HiddenImpl class. In doing so, you eliminate the need to pass a back pointer to access members in the VisibleClass from HiddenImpl. Unfortunately you cannot hide virtual members in the member functions of HiddenImpl.When to use?There are several reasons why you might want to use PIMPL in your code:  Decrease Compilation timesWhenever the header file of class X changes, every client that uses X has to be recompiled. With PIMPL, only the implementation of X has to be recompiled when you makes changes to the underlying fields/methods.  Binary CompatibilityAdding new fields/methods to HiddenImpl will not affect the binary interface your end user sees. So changes are less high risk.  Hiding Implementations/DataBecause your source files will be compiled into binaries/libraries, when you share your headers and librar (.so or .a file) you will have hidden part of your implementation from the end user.Sources:  PImpl - cppreference.com  PImpl Idiom in C++ with Examples - GeeksforGeeks  c++ - Is the pImpl idiom really used in practice? - Stack Overflow  How to implement the pimpl idiom by using unique_ptr - Fluent C++  GotW #100: Compilation Firewalls (Difficulty: 6/10) – Sutter’s Mill"
  },
  
  {
    "title": "Cpp Blog: Cpp Casts",
    "url": "/posts/cpp-casts/",
    "categories": "Cpp Blog",
    "tags": "cpp",
    "date": "2020-09-05 00:00:00 +0800",
    





    "snippet": "Medium Link"
  },
  
  {
    "title": "Software Blog: OOP Basics",
    "url": "/posts/oop-basics/",
    "categories": "Software Blog",
    "tags": "oop",
    "date": "2020-05-05 00:00:00 +0800",
    





    "snippet": "Object-oriented programming (OOP) is programming paradigm based on based on wrapping data and behaviour into bundles (or objects) that enables our code to be more modular, flexible and reusable. Classes are the “blueprints” that define the structure(states and behaviours) of our objects and objects are concrete instances of our classes. There are 4 key aspects of OOP that differentiate it from other paradigms. I used the acronym IPEA (like IKEA with a P) to remember the four:  Abstraction  Encapsulation  Inheritance  PolymorphismAbstraction is the act of modelling a real world object or phenomenon to a limited extent that is relevant to our context. Our context determines what to model. For instance, if you were modelling a person in a healthcare database, you would include fields such as height, weight and past medical history. On the other hand, if you were to model someone in an employee database you might opt include fields such as salary, position and peer reviews.Encapsulation is the ability to expose a limited interface to the rest of a program. In essence, it is hiding parts of its state and behaviours from other objects and clients. As an example, a mobile robot API designer will encapsulate all the code that goes into making a robot move (e.g. starting up obscure hardware modules, sending PWM signals to motors) and only expose a limited interface (e.g. move left, right forward or backward) to its client because moving the robot is the only thing of interest to the client.Inheritance is the ability for new classes to build on top of existing ones to help us maximize code reuse. You might have an Animal parent class (superclass, with Dog and Cat children classes (subclass). Your subclasses would then inherit the behaviours (e.g. eat() and sleep() ) and attributes (e.g. weight and blood_type )of the Animal class.Polymorphism is the ability of an object to take on multiple forms. More concretely, it is the ability of a program to call the underlying implementation of a program without knowing its true form. We may have a pointer to an Animal object which could be a Dog or a Cat . Regardless of dog/cat, we can call eat() and the program will help us determine what the object should eat (e.g. dog food vs. cat food) without us having to worry about the underlying animal type.Summary:Abstraction = model real world object/phenomenon to a limited extentEncapsulation = exposing a limited interface to the rest of the programInheritance = ability for new classes to build on existing onesPolymorphoism = ability for a base object to take on different forms"
  },
  
  {
    "title": "Software Blog: UML Basics Reference",
    "url": "/posts/uml-basics-reference/",
    "categories": "Software Blog",
    "tags": "uml, oop",
    "date": "2020-04-04 00:00:00 +0800",
    





    "snippet": "Medium Link"
  },
  
  {
    "title": "Cpp Blog: Smart Pointers in C++",
    "url": "/posts/smart-pointers-in-cpp/",
    "categories": "Cpp Blog",
    "tags": "cpp, smart pointers",
    "date": "2020-04-03 00:00:00 +0800",
    





    "snippet": "Medium Link"
  }
  
]

